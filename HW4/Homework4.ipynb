{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>\n",
    "<center>CFRM 421/521, Spring 2022</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Shengbo Jin</center>\n",
    "</h1>\n",
    "\n",
    "<h1>\n",
    "<center>Homework 4</center>\n",
    "</h1>\n",
    "\n",
    "* **Due: Tuesday, May 31, 2022, 11:59 PM**\n",
    "\n",
    "\n",
    "* Total marks: 47\n",
    "\n",
    "\n",
    "* Late submissions are allowed, but a 20% penalty per day applies. Your last submission is considered for calculating the penalty.\n",
    "\n",
    "\n",
    "*  Use this Jupyter notebook as a template for your solutions. **Your solution must be submitted as one Jupyter notebook on Canvas and one PDF file on Gradescope.** The notebook must be already run, that is, make sure that you have run all your code, save the notebook, and then when you reopen the notebook, checked that all output appears as expected. You are allowed to use code from the textbook, textbook website, or lecture notes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. A regression MLP [10 marks]\n",
    "\n",
    "Consider the California housing data from Homework 1 using the same training and test set there. Here, we split off 20% of the training set as a validation set, and keep the remaining 80% as the actual training set.  The following code replicates the preprocessing of the dataset from Homework 1, creating the training set `X_train`, `y_train`, the validation set `X_valid`, `y_valid` and the test set `X_test`, `y_test`. The target variable has been divided by 100,000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "\n",
    "def fetch_housing_data(housing_url, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)\n",
    "\n",
    "HOUSING_URL = (\"https://raw.githubusercontent.com/ageron/\"+\n",
    "               \"handson-ml2/master/datasets/housing/housing.tgz\")\n",
    "fetch_housing_data(HOUSING_URL)\n",
    "data = load_housing_data()\n",
    "\n",
    "data[\"income_cat\"] = np.ceil(data[\"median_income\"] / 1.5)\n",
    "data[\"income_cat\"].where(data[\"income_cat\"] < 5, 5.0, inplace=True)\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"income_cat\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "    \n",
    "# Split the traning set into training and validation    \n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index2, valid_index in split2.split(strat_train_set, strat_train_set[\"income_cat\"]):\n",
    "    strat_train2_set = strat_train_set.iloc[train_index2]\n",
    "    strat_valid_set = strat_train_set.iloc[valid_index]\n",
    "    \n",
    "strat_train_set = strat_train2_set.copy().drop(\"income_cat\", axis=1)\n",
    "strat_valid_set = strat_valid_set.copy().drop(\"income_cat\", axis=1)\n",
    "strat_test_set = strat_test_set.copy().drop(\"income_cat\", axis=1)\n",
    "\n",
    "X_raw = strat_train_set.drop(\"median_house_value\", axis=1)\n",
    "y_train = strat_train_set[\"median_house_value\"].copy()/100000\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "num_features = X_raw.drop(\"ocean_proximity\", axis=1)\n",
    "num_attribs = list(num_features)\n",
    "cat_attribs = [\"ocean_proximity\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "# Apply the pipeline to the training set\n",
    "X_train = full_pipeline.fit_transform(X_raw)\n",
    "\n",
    "# Apply the pipeline to the validation set\n",
    "X_valid_raw = strat_valid_set.drop(\"median_house_value\", axis=1)\n",
    "y_valid = strat_valid_set[\"median_house_value\"].copy()/100000\n",
    "X_valid = full_pipeline.transform(X_valid_raw)\n",
    "\n",
    "# Apply the pipeline to the validation set\n",
    "X_test_raw = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()/100000\n",
    "X_test = full_pipeline.transform(X_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [4 marks]\n",
    "\n",
    "Use `tensorflow.keras` to train a regression MLP with one hidden layer of 50 ReLU neurons. For the output layer, try both a ReLU activation function and no activation function (which is equivalent to the identity function). Explain which choice is better. Use the appropriate weight initialization. Use the Nadam optimizer. Train for 30 epochs, and report the mean squared error on the validation set.\n",
    "\n",
    "Hint: In the `.compile()` method, use `loss=\"mse\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ReLU activation function for the output layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "\n",
    "def reset_session(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.backend.clear_session()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "413/413 [==============================] - 1s 867us/step - loss: 1.2245 - mse: 1.2245 - val_loss: 0.5247 - val_mse: 0.5247\n",
      "Epoch 2/30\n",
      "413/413 [==============================] - 0s 630us/step - loss: 0.4593 - mse: 0.4593 - val_loss: 0.4519 - val_mse: 0.4519\n",
      "Epoch 3/30\n",
      "413/413 [==============================] - 0s 617us/step - loss: 0.4132 - mse: 0.4132 - val_loss: 0.4150 - val_mse: 0.4150\n",
      "Epoch 4/30\n",
      "413/413 [==============================] - 0s 639us/step - loss: 0.3927 - mse: 0.3927 - val_loss: 0.3970 - val_mse: 0.3970\n",
      "Epoch 5/30\n",
      "413/413 [==============================] - 0s 632us/step - loss: 0.3811 - mse: 0.3811 - val_loss: 0.3847 - val_mse: 0.3847\n",
      "Epoch 6/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3725 - mse: 0.3725 - val_loss: 0.3765 - val_mse: 0.3765\n",
      "Epoch 7/30\n",
      "413/413 [==============================] - 0s 623us/step - loss: 0.3664 - mse: 0.3664 - val_loss: 0.3754 - val_mse: 0.3754\n",
      "Epoch 8/30\n",
      "413/413 [==============================] - 0s 649us/step - loss: 0.3611 - mse: 0.3611 - val_loss: 0.3676 - val_mse: 0.3676\n",
      "Epoch 9/30\n",
      "413/413 [==============================] - 0s 626us/step - loss: 0.3569 - mse: 0.3569 - val_loss: 0.3644 - val_mse: 0.3644\n",
      "Epoch 10/30\n",
      "413/413 [==============================] - 0s 612us/step - loss: 0.3530 - mse: 0.3530 - val_loss: 0.3595 - val_mse: 0.3595\n",
      "Epoch 11/30\n",
      "413/413 [==============================] - 0s 634us/step - loss: 0.3491 - mse: 0.3491 - val_loss: 0.3571 - val_mse: 0.3571\n",
      "Epoch 12/30\n",
      "413/413 [==============================] - 0s 623us/step - loss: 0.3456 - mse: 0.3456 - val_loss: 0.3548 - val_mse: 0.3548\n",
      "Epoch 13/30\n",
      "413/413 [==============================] - 0s 621us/step - loss: 0.3424 - mse: 0.3424 - val_loss: 0.3564 - val_mse: 0.3564\n",
      "Epoch 14/30\n",
      "413/413 [==============================] - 0s 645us/step - loss: 0.3408 - mse: 0.3408 - val_loss: 0.3514 - val_mse: 0.3514\n",
      "Epoch 15/30\n",
      "413/413 [==============================] - 0s 625us/step - loss: 0.3383 - mse: 0.3383 - val_loss: 0.3489 - val_mse: 0.3489\n",
      "Epoch 16/30\n",
      "413/413 [==============================] - 0s 624us/step - loss: 0.3354 - mse: 0.3354 - val_loss: 0.3471 - val_mse: 0.3471\n",
      "Epoch 17/30\n",
      "413/413 [==============================] - 0s 615us/step - loss: 0.3328 - mse: 0.3328 - val_loss: 0.3471 - val_mse: 0.3471\n",
      "Epoch 18/30\n",
      "413/413 [==============================] - 0s 621us/step - loss: 0.3323 - mse: 0.3323 - val_loss: 0.3459 - val_mse: 0.3459\n",
      "Epoch 19/30\n",
      "413/413 [==============================] - 0s 667us/step - loss: 0.3304 - mse: 0.3304 - val_loss: 0.3418 - val_mse: 0.3418\n",
      "Epoch 20/30\n",
      "413/413 [==============================] - 0s 653us/step - loss: 0.3287 - mse: 0.3287 - val_loss: 0.3456 - val_mse: 0.3456\n",
      "Epoch 21/30\n",
      "413/413 [==============================] - 0s 633us/step - loss: 0.3280 - mse: 0.3280 - val_loss: 0.3400 - val_mse: 0.3400\n",
      "Epoch 22/30\n",
      "413/413 [==============================] - 0s 624us/step - loss: 0.3270 - mse: 0.3270 - val_loss: 0.3393 - val_mse: 0.3393\n",
      "Epoch 23/30\n",
      "413/413 [==============================] - 0s 619us/step - loss: 0.3260 - mse: 0.3260 - val_loss: 0.3434 - val_mse: 0.3434\n",
      "Epoch 24/30\n",
      "413/413 [==============================] - 0s 632us/step - loss: 0.3248 - mse: 0.3248 - val_loss: 0.3376 - val_mse: 0.3376\n",
      "Epoch 25/30\n",
      "413/413 [==============================] - 0s 618us/step - loss: 0.3238 - mse: 0.3238 - val_loss: 0.3397 - val_mse: 0.3397\n",
      "Epoch 26/30\n",
      "413/413 [==============================] - 0s 629us/step - loss: 0.3215 - mse: 0.3215 - val_loss: 0.3388 - val_mse: 0.3388\n",
      "Epoch 27/30\n",
      "413/413 [==============================] - 0s 640us/step - loss: 0.3213 - mse: 0.3213 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 28/30\n",
      "413/413 [==============================] - 0s 619us/step - loss: 0.3202 - mse: 0.3202 - val_loss: 0.3352 - val_mse: 0.3352\n",
      "Epoch 29/30\n",
      "413/413 [==============================] - 0s 621us/step - loss: 0.3204 - mse: 0.3204 - val_loss: 0.3371 - val_mse: 0.3371\n",
      "Epoch 30/30\n",
      "413/413 [==============================] - 0s 609us/step - loss: 0.3192 - mse: 0.3192 - val_loss: 0.3386 - val_mse: 0.3386\n",
      "104/104 [==============================] - 0s 461us/step - loss: 0.3386 - mse: 0.3386\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.33857136964797974, 0.33857136964797974]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1, activation=\"relu\", kernel_initializer=\"he_normal\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mse\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30,\n",
    "          validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### No activation function for the output layer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "413/413 [==============================] - 1s 792us/step - loss: 0.7316 - mse: 0.7316 - val_loss: 0.5167 - val_mse: 0.5167\n",
      "Epoch 2/30\n",
      "413/413 [==============================] - 0s 641us/step - loss: 0.4489 - mse: 0.4489 - val_loss: 0.4504 - val_mse: 0.4504\n",
      "Epoch 3/30\n",
      "413/413 [==============================] - 0s 691us/step - loss: 0.4092 - mse: 0.4092 - val_loss: 0.4174 - val_mse: 0.4174\n",
      "Epoch 4/30\n",
      "413/413 [==============================] - 0s 693us/step - loss: 0.3914 - mse: 0.3914 - val_loss: 0.4017 - val_mse: 0.4017\n",
      "Epoch 5/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3808 - mse: 0.3808 - val_loss: 0.3905 - val_mse: 0.3905\n",
      "Epoch 6/30\n",
      "413/413 [==============================] - 0s 643us/step - loss: 0.3724 - mse: 0.3724 - val_loss: 0.3824 - val_mse: 0.3824\n",
      "Epoch 7/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3660 - mse: 0.3660 - val_loss: 0.3805 - val_mse: 0.3805\n",
      "Epoch 8/30\n",
      "413/413 [==============================] - 0s 643us/step - loss: 0.3604 - mse: 0.3604 - val_loss: 0.3729 - val_mse: 0.3729\n",
      "Epoch 9/30\n",
      "413/413 [==============================] - 0s 656us/step - loss: 0.3567 - mse: 0.3567 - val_loss: 0.3670 - val_mse: 0.3670\n",
      "Epoch 10/30\n",
      "413/413 [==============================] - 0s 645us/step - loss: 0.3526 - mse: 0.3526 - val_loss: 0.3658 - val_mse: 0.3658\n",
      "Epoch 11/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3493 - mse: 0.3493 - val_loss: 0.3611 - val_mse: 0.3611\n",
      "Epoch 12/30\n",
      "413/413 [==============================] - 0s 629us/step - loss: 0.3465 - mse: 0.3465 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 13/30\n",
      "413/413 [==============================] - 0s 637us/step - loss: 0.3435 - mse: 0.3435 - val_loss: 0.3590 - val_mse: 0.3590\n",
      "Epoch 14/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3425 - mse: 0.3425 - val_loss: 0.3561 - val_mse: 0.3561\n",
      "Epoch 15/30\n",
      "413/413 [==============================] - 0s 636us/step - loss: 0.3398 - mse: 0.3398 - val_loss: 0.3540 - val_mse: 0.3540\n",
      "Epoch 16/30\n",
      "413/413 [==============================] - 0s 629us/step - loss: 0.3372 - mse: 0.3372 - val_loss: 0.3497 - val_mse: 0.3497\n",
      "Epoch 17/30\n",
      "413/413 [==============================] - 0s 637us/step - loss: 0.3348 - mse: 0.3348 - val_loss: 0.3515 - val_mse: 0.3515\n",
      "Epoch 18/30\n",
      "413/413 [==============================] - 0s 654us/step - loss: 0.3344 - mse: 0.3344 - val_loss: 0.3489 - val_mse: 0.3489\n",
      "Epoch 19/30\n",
      "413/413 [==============================] - 0s 638us/step - loss: 0.3322 - mse: 0.3322 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 20/30\n",
      "413/413 [==============================] - 0s 643us/step - loss: 0.3302 - mse: 0.3302 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 21/30\n",
      "413/413 [==============================] - 0s 641us/step - loss: 0.3292 - mse: 0.3292 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 22/30\n",
      "413/413 [==============================] - 0s 648us/step - loss: 0.3284 - mse: 0.3284 - val_loss: 0.3412 - val_mse: 0.3412\n",
      "Epoch 23/30\n",
      "413/413 [==============================] - 0s 641us/step - loss: 0.3269 - mse: 0.3269 - val_loss: 0.3434 - val_mse: 0.3434\n",
      "Epoch 24/30\n",
      "413/413 [==============================] - 0s 684us/step - loss: 0.3252 - mse: 0.3252 - val_loss: 0.3390 - val_mse: 0.3390\n",
      "Epoch 25/30\n",
      "413/413 [==============================] - 0s 673us/step - loss: 0.3240 - mse: 0.3240 - val_loss: 0.3413 - val_mse: 0.3413\n",
      "Epoch 26/30\n",
      "413/413 [==============================] - 0s 684us/step - loss: 0.3212 - mse: 0.3212 - val_loss: 0.3421 - val_mse: 0.3421\n",
      "Epoch 27/30\n",
      "413/413 [==============================] - 0s 653us/step - loss: 0.3207 - mse: 0.3207 - val_loss: 0.3483 - val_mse: 0.3483\n",
      "Epoch 28/30\n",
      "413/413 [==============================] - 0s 663us/step - loss: 0.3191 - mse: 0.3191 - val_loss: 0.3373 - val_mse: 0.3373\n",
      "Epoch 29/30\n",
      "413/413 [==============================] - 0s 665us/step - loss: 0.3197 - mse: 0.3197 - val_loss: 0.3372 - val_mse: 0.3372\n",
      "Epoch 30/30\n",
      "413/413 [==============================] - 0s 655us/step - loss: 0.3178 - mse: 0.3178 - val_loss: 0.3393 - val_mse: 0.3393\n",
      "104/104 [==============================] - 0s 463us/step - loss: 0.3393 - mse: 0.3393\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.339341938495636, 0.339341938495636]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(50, activation=\"relu\", kernel_initializer=\"he_normal\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\")\n",
    "])\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=0.001, beta_1=0.9, beta_2=0.999)\n",
    "model.compile(loss=\"mse\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mse\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=30,\n",
    "          validation_data=(X_valid, y_valid))\n",
    "\n",
    "model.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [5 marks]\n",
    "\n",
    "Read the section \"Fine-Tuning Neural Network Hyperparameters\" in the textbook and the corresponding section in the [Jupyter notebook](https://github.com/ageron/handson-ml2/blob/master/10_neural_nets_with_keras.ipynb) on the textbook website. Then use a randomized search to search for the best number of hidden layers, neurons per hidden layer, and learning rate. For the randomized search use 3-fold CV, with 10 iterations, with the number of hidden layers uniformly sampled from $\\{0,1,2,3\\}$, the number of neurons per layer uniformly from $\\{1,2,\\dots,100\\}$, and the learning rate from the distribution `reciprocal(3e-4, 3e-2)`. Use early stopping with `patience=10`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=X_train.shape[1:]):\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.InputLayer(input_shape=input_shape))\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.Dense(1, kernel_initializer=\"glorot_uniform\"))\n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.999)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer, metrics=[\"mse\"])\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King48\\AppData\\Local\\Temp\\ipykernel_121048\\4087176523.py:1: DeprecationWarning: KerasRegressor is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)\n"
     ]
    }
   ],
   "source": [
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 942us/step - loss: 0.6828 - mse: 0.6828 - val_loss: 0.4731 - val_mse: 0.4731\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 715us/step - loss: 0.4449 - mse: 0.4449 - val_loss: 0.4480 - val_mse: 0.4480\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.4357 - mse: 0.4357 - val_loss: 0.4559 - val_mse: 0.4559\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 725us/step - loss: 0.4279 - mse: 0.4279 - val_loss: 0.4210 - val_mse: 0.4210\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4265 - mse: 0.4265 - val_loss: 0.5548 - val_mse: 0.5548\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.4292 - mse: 0.4292 - val_loss: 0.4418 - val_mse: 0.4418\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.4267 - mse: 0.4267 - val_loss: 0.4240 - val_mse: 0.4240\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.4235 - mse: 0.4235 - val_loss: 0.4485 - val_mse: 0.4485\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4204 - mse: 0.4204 - val_loss: 0.4212 - val_mse: 0.4212\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 691us/step - loss: 0.4207 - mse: 0.4207 - val_loss: 0.4214 - val_mse: 0.4214\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.4224 - mse: 0.4224 - val_loss: 0.4983 - val_mse: 0.4983\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.4214 - mse: 0.4214 - val_loss: 0.4140 - val_mse: 0.4140\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 726us/step - loss: 0.4188 - mse: 0.4188 - val_loss: 0.4204 - val_mse: 0.4204\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 735us/step - loss: 0.4183 - mse: 0.4183 - val_loss: 0.4399 - val_mse: 0.4399\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 742us/step - loss: 0.4159 - mse: 0.4159 - val_loss: 0.4309 - val_mse: 0.4309\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.4190 - mse: 0.4190 - val_loss: 0.4242 - val_mse: 0.4242\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 729us/step - loss: 0.4178 - mse: 0.4178 - val_loss: 0.4139 - val_mse: 0.4139\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 725us/step - loss: 0.4202 - mse: 0.4202 - val_loss: 0.4207 - val_mse: 0.4207\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.4180 - mse: 0.4180 - val_loss: 0.4387 - val_mse: 0.4387\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 723us/step - loss: 0.4192 - mse: 0.4192 - val_loss: 0.4212 - val_mse: 0.4212\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.4161 - mse: 0.4161 - val_loss: 0.4170 - val_mse: 0.4170\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 704us/step - loss: 0.4174 - mse: 0.4174 - val_loss: 0.4202 - val_mse: 0.4202\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4141 - mse: 0.4141 - val_loss: 0.4120 - val_mse: 0.4120\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.4174 - mse: 0.4174 - val_loss: 0.4126 - val_mse: 0.4126\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.4151 - mse: 0.4151 - val_loss: 0.4077 - val_mse: 0.4077\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 732us/step - loss: 0.4145 - mse: 0.4145 - val_loss: 0.4633 - val_mse: 0.4633\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 734us/step - loss: 0.4109 - mse: 0.4109 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.4125 - mse: 0.4125 - val_loss: 0.4247 - val_mse: 0.4247\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.4121 - mse: 0.4121 - val_loss: 0.4086 - val_mse: 0.4086\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 704us/step - loss: 0.4078 - mse: 0.4078 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "138/138 [==============================] - 0s 462us/step - loss: 0.4277 - mse: 0.4277\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   6.4s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 932us/step - loss: 0.7160 - mse: 0.7160 - val_loss: 0.4890 - val_mse: 0.4890\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4447 - mse: 0.4447 - val_loss: 0.4488 - val_mse: 0.4488\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.4294 - mse: 0.4294 - val_loss: 0.4413 - val_mse: 0.4413\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 717us/step - loss: 0.4191 - mse: 0.4191 - val_loss: 0.4390 - val_mse: 0.4390\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 717us/step - loss: 0.4168 - mse: 0.4168 - val_loss: 0.5443 - val_mse: 0.5443\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.4136 - mse: 0.4136 - val_loss: 0.4367 - val_mse: 0.4367\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 797us/step - loss: 0.4084 - mse: 0.4084 - val_loss: 0.4284 - val_mse: 0.4284\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 734us/step - loss: 0.4072 - mse: 0.4072 - val_loss: 0.4216 - val_mse: 0.4216\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.4031 - mse: 0.4031 - val_loss: 0.4116 - val_mse: 0.4116\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 726us/step - loss: 0.4042 - mse: 0.4042 - val_loss: 0.4235 - val_mse: 0.4235\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 739us/step - loss: 0.3998 - mse: 0.3998 - val_loss: 0.4154 - val_mse: 0.4154\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 718us/step - loss: 0.3974 - mse: 0.3974 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.4001 - mse: 0.4001 - val_loss: 0.4343 - val_mse: 0.4343\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 694us/step - loss: 0.3959 - mse: 0.3959 - val_loss: 0.4042 - val_mse: 0.4042\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.3980 - mse: 0.3980 - val_loss: 0.4072 - val_mse: 0.4072\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 688us/step - loss: 0.3966 - mse: 0.3966 - val_loss: 0.4884 - val_mse: 0.4884\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 741us/step - loss: 0.4002 - mse: 0.4002 - val_loss: 0.4132 - val_mse: 0.4132\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 707us/step - loss: 0.3962 - mse: 0.3962 - val_loss: 0.4066 - val_mse: 0.4066\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 699us/step - loss: 0.3957 - mse: 0.3957 - val_loss: 0.4013 - val_mse: 0.4013\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 729us/step - loss: 0.3942 - mse: 0.3942 - val_loss: 0.4047 - val_mse: 0.4047\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 692us/step - loss: 0.3916 - mse: 0.3916 - val_loss: 0.4335 - val_mse: 0.4335\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 690us/step - loss: 0.3934 - mse: 0.3934 - val_loss: 0.4176 - val_mse: 0.4176\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 715us/step - loss: 0.3965 - mse: 0.3965 - val_loss: 0.4187 - val_mse: 0.4187\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.3937 - mse: 0.3937 - val_loss: 0.4215 - val_mse: 0.4215\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 687us/step - loss: 0.3950 - mse: 0.3950 - val_loss: 0.4014 - val_mse: 0.4014\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 695us/step - loss: 0.3945 - mse: 0.3945 - val_loss: 0.4107 - val_mse: 0.4107\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.3928 - mse: 0.3928 - val_loss: 0.4292 - val_mse: 0.4292\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.3933 - mse: 0.3933 - val_loss: 0.4087 - val_mse: 0.4087\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 689us/step - loss: 0.3921 - mse: 0.3921 - val_loss: 0.4018 - val_mse: 0.4018\n",
      "138/138 [==============================] - 0s 430us/step - loss: 0.4011 - mse: 0.4011\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   6.2s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 939us/step - loss: 0.7920 - mse: 0.7920 - val_loss: 0.4795 - val_mse: 0.4795\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 695us/step - loss: 0.4591 - mse: 0.4591 - val_loss: 0.4587 - val_mse: 0.4587\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 684us/step - loss: 0.4513 - mse: 0.4513 - val_loss: 0.4655 - val_mse: 0.4655\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.4421 - mse: 0.4421 - val_loss: 0.4445 - val_mse: 0.4445\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 685us/step - loss: 0.4424 - mse: 0.4424 - val_loss: 0.4348 - val_mse: 0.4348\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 686us/step - loss: 0.4355 - mse: 0.4355 - val_loss: 0.4497 - val_mse: 0.4497\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 697us/step - loss: 0.4259 - mse: 0.4259 - val_loss: 0.4599 - val_mse: 0.4599\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 683us/step - loss: 0.4133 - mse: 0.4133 - val_loss: 0.4153 - val_mse: 0.4153\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.4059 - mse: 0.4059 - val_loss: 0.3989 - val_mse: 0.3989\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 695us/step - loss: 0.4029 - mse: 0.4029 - val_loss: 0.4386 - val_mse: 0.4386\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 686us/step - loss: 0.4008 - mse: 0.4008 - val_loss: 0.3934 - val_mse: 0.3934\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 687us/step - loss: 0.3979 - mse: 0.3979 - val_loss: 0.3895 - val_mse: 0.3895\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 692us/step - loss: 0.4008 - mse: 0.4008 - val_loss: 0.4239 - val_mse: 0.4239\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 686us/step - loss: 0.4019 - mse: 0.4019 - val_loss: 0.3963 - val_mse: 0.3963\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 686us/step - loss: 0.3970 - mse: 0.3970 - val_loss: 0.3946 - val_mse: 0.3946\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 707us/step - loss: 0.3985 - mse: 0.3985 - val_loss: 0.6587 - val_mse: 0.6587\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.4156 - mse: 0.4156 - val_loss: 0.4068 - val_mse: 0.4068\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 689us/step - loss: 0.3969 - mse: 0.3969 - val_loss: 0.3926 - val_mse: 0.3926\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.3883 - val_mse: 0.3883\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 690us/step - loss: 0.3949 - mse: 0.3949 - val_loss: 0.4042 - val_mse: 0.4042\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.3933 - mse: 0.3933 - val_loss: 0.4016 - val_mse: 0.4016\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.3965 - mse: 0.3965 - val_loss: 0.4020 - val_mse: 0.4020\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.3963 - mse: 0.3963 - val_loss: 0.3808 - val_mse: 0.3808\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 704us/step - loss: 0.3933 - mse: 0.3933 - val_loss: 0.4249 - val_mse: 0.4249\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 696us/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.4045 - val_mse: 0.4045\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 696us/step - loss: 0.3951 - mse: 0.3951 - val_loss: 0.4329 - val_mse: 0.4329\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3943 - mse: 0.3943 - val_loss: 0.3845 - val_mse: 0.3845\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.3938 - mse: 0.3938 - val_loss: 0.3866 - val_mse: 0.3866\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 704us/step - loss: 0.3943 - mse: 0.3943 - val_loss: 0.3972 - val_mse: 0.3972\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 720us/step - loss: 0.3929 - mse: 0.3929 - val_loss: 0.3843 - val_mse: 0.3843\n",
      "138/138 [==============================] - 0s 459us/step - loss: 0.3801 - mse: 0.3801\n",
      "[CV] END learning_rate=0.022174573948353458, n_hidden=1, n_neurons=4; total time=   6.4s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5846 - mse: 0.5846 - val_loss: 0.4801 - val_mse: 0.4801\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 793us/step - loss: 0.3958 - mse: 0.3958 - val_loss: 0.3940 - val_mse: 0.3940\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 802us/step - loss: 0.3651 - mse: 0.3651 - val_loss: 0.3752 - val_mse: 0.3752\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.3641 - mse: 0.3641 - val_loss: 0.3399 - val_mse: 0.3399\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.3360 - mse: 0.3360 - val_loss: 0.6142 - val_mse: 0.6142\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 794us/step - loss: 0.3418 - mse: 0.3418 - val_loss: 0.3777 - val_mse: 0.3777\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 791us/step - loss: 0.3351 - mse: 0.3351 - val_loss: 0.3376 - val_mse: 0.3376\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.3158 - mse: 0.3158 - val_loss: 0.3835 - val_mse: 0.3835\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 773us/step - loss: 0.3061 - mse: 0.3061 - val_loss: 0.3278 - val_mse: 0.3278\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.3013 - mse: 0.3013 - val_loss: 0.3629 - val_mse: 0.3629\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 787us/step - loss: 0.3003 - mse: 0.3003 - val_loss: 0.4509 - val_mse: 0.4509\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.3000 - mse: 0.3000 - val_loss: 0.3081 - val_mse: 0.3081\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2946 - mse: 0.2946 - val_loss: 0.3221 - val_mse: 0.3221\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 813us/step - loss: 0.2884 - mse: 0.2884 - val_loss: 0.3667 - val_mse: 0.3667\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.2843 - mse: 0.2843 - val_loss: 0.3259 - val_mse: 0.3259\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2844 - mse: 0.2844 - val_loss: 0.3482 - val_mse: 0.3482\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2807 - mse: 0.2807 - val_loss: 0.3037 - val_mse: 0.3037\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2785 - mse: 0.2785 - val_loss: 0.3369 - val_mse: 0.3369\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 783us/step - loss: 0.2739 - mse: 0.2739 - val_loss: 0.3128 - val_mse: 0.3128\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2707 - mse: 0.2707 - val_loss: 0.3351 - val_mse: 0.3351\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.2678 - mse: 0.2678 - val_loss: 0.3115 - val_mse: 0.3115\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 788us/step - loss: 0.2632 - mse: 0.2632 - val_loss: 0.3160 - val_mse: 0.3160\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2645 - mse: 0.2645 - val_loss: 0.3212 - val_mse: 0.3212\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 788us/step - loss: 0.2628 - mse: 0.2628 - val_loss: 0.3158 - val_mse: 0.3158\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 807us/step - loss: 0.2597 - mse: 0.2597 - val_loss: 0.3144 - val_mse: 0.3144\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2594 - mse: 0.2594 - val_loss: 0.4263 - val_mse: 0.4263\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.2568 - mse: 0.2568 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "138/138 [==============================] - 0s 461us/step - loss: 0.3001 - mse: 0.3001\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   6.5s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.6075 - mse: 0.6075 - val_loss: 0.4686 - val_mse: 0.4686\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.3953 - mse: 0.3953 - val_loss: 0.3824 - val_mse: 0.3824\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 843us/step - loss: 0.3634 - mse: 0.3634 - val_loss: 0.3625 - val_mse: 0.3625\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 796us/step - loss: 0.3487 - mse: 0.3487 - val_loss: 0.3606 - val_mse: 0.3606\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 889us/step - loss: 0.3251 - mse: 0.3251 - val_loss: 0.6057 - val_mse: 0.6057\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.3157 - mse: 0.3157 - val_loss: 0.3804 - val_mse: 0.3804\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.3042 - mse: 0.3042 - val_loss: 0.3623 - val_mse: 0.3623\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2982 - mse: 0.2982 - val_loss: 0.3505 - val_mse: 0.3505\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.3227 - val_mse: 0.3227\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.2896 - mse: 0.2896 - val_loss: 0.3274 - val_mse: 0.3274\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.2832 - mse: 0.2832 - val_loss: 0.3078 - val_mse: 0.3078\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 752us/step - loss: 0.2809 - mse: 0.2809 - val_loss: 0.3072 - val_mse: 0.3072\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2767 - mse: 0.2767 - val_loss: 0.3208 - val_mse: 0.3208\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 816us/step - loss: 0.2769 - mse: 0.2769 - val_loss: 0.3213 - val_mse: 0.3213\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.2709 - mse: 0.2709 - val_loss: 0.3103 - val_mse: 0.3103\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 794us/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.2959 - val_mse: 0.2959\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 816us/step - loss: 0.2636 - mse: 0.2636 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2633 - mse: 0.2633 - val_loss: 0.3244 - val_mse: 0.3244\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 837us/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.3046 - val_mse: 0.3046\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2555 - mse: 0.2555 - val_loss: 0.3069 - val_mse: 0.3069\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 797us/step - loss: 0.2493 - mse: 0.2493 - val_loss: 0.3309 - val_mse: 0.3309\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.2525 - mse: 0.2525 - val_loss: 0.3042 - val_mse: 0.3042\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.2982 - val_mse: 0.2982\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 787us/step - loss: 0.2486 - mse: 0.2486 - val_loss: 0.3121 - val_mse: 0.3121\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2424 - mse: 0.2424 - val_loss: 0.3148 - val_mse: 0.3148\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2471 - mse: 0.2471 - val_loss: 0.3323 - val_mse: 0.3323\n",
      "138/138 [==============================] - 0s 460us/step - loss: 0.3384 - mse: 0.3384\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   6.2s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5540 - mse: 0.5540 - val_loss: 0.4052 - val_mse: 0.4052\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.3768 - mse: 0.3768 - val_loss: 0.3893 - val_mse: 0.3893\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.3576 - mse: 0.3576 - val_loss: 0.3747 - val_mse: 0.3747\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3393 - mse: 0.3393 - val_loss: 0.3427 - val_mse: 0.3427\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 766us/step - loss: 0.3270 - mse: 0.3270 - val_loss: 0.3610 - val_mse: 0.3610\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 773us/step - loss: 0.3208 - mse: 0.3208 - val_loss: 0.3735 - val_mse: 0.3735\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 842us/step - loss: 0.3152 - mse: 0.3152 - val_loss: 0.4104 - val_mse: 0.4104\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.3109 - mse: 0.3109 - val_loss: 0.3480 - val_mse: 0.3480\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.3011 - mse: 0.3011 - val_loss: 0.3356 - val_mse: 0.3356\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2955 - mse: 0.2955 - val_loss: 0.3254 - val_mse: 0.3254\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 761us/step - loss: 0.2958 - mse: 0.2958 - val_loss: 0.3204 - val_mse: 0.3204\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.3264 - val_mse: 0.3264\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 773us/step - loss: 0.2830 - mse: 0.2830 - val_loss: 0.3179 - val_mse: 0.3179\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.2848 - mse: 0.2848 - val_loss: 0.3348 - val_mse: 0.3348\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 793us/step - loss: 0.2776 - mse: 0.2776 - val_loss: 0.3343 - val_mse: 0.3343\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.2731 - mse: 0.2731 - val_loss: 0.3316 - val_mse: 0.3316\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 784us/step - loss: 0.2765 - mse: 0.2765 - val_loss: 0.2959 - val_mse: 0.2959\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2690 - mse: 0.2690 - val_loss: 0.3302 - val_mse: 0.3302\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.2667 - mse: 0.2667 - val_loss: 0.3131 - val_mse: 0.3131\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2642 - mse: 0.2642 - val_loss: 0.3350 - val_mse: 0.3350\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.2587 - mse: 0.2587 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2576 - mse: 0.2576 - val_loss: 0.3229 - val_mse: 0.3229\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 783us/step - loss: 0.2576 - mse: 0.2576 - val_loss: 0.3183 - val_mse: 0.3183\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.3117 - val_mse: 0.3117\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.2517 - mse: 0.2517 - val_loss: 0.3475 - val_mse: 0.3475\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 847us/step - loss: 0.2521 - mse: 0.2521 - val_loss: 0.3770 - val_mse: 0.3770\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 799us/step - loss: 0.2474 - mse: 0.2474 - val_loss: 0.3129 - val_mse: 0.3129\n",
      "138/138 [==============================] - 0s 494us/step - loss: 0.3038 - mse: 0.3038\n",
      "[CV] END learning_rate=0.005432590230265343, n_hidden=2, n_neurons=94; total time=   6.4s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 934us/step - loss: 3.2374 - mse: 3.2374 - val_loss: 1.3619 - val_mse: 1.3619\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 728us/step - loss: 1.0763 - mse: 1.0763 - val_loss: 0.9564 - val_mse: 0.9564\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 731us/step - loss: 0.8159 - mse: 0.8159 - val_loss: 0.8006 - val_mse: 0.8006\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.6977 - mse: 0.6977 - val_loss: 0.7021 - val_mse: 0.7021\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 726us/step - loss: 0.6244 - mse: 0.6244 - val_loss: 0.6375 - val_mse: 0.6375\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.5717 - mse: 0.5717 - val_loss: 0.5877 - val_mse: 0.5877\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.5317 - mse: 0.5317 - val_loss: 0.5511 - val_mse: 0.5511\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 720us/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5222 - val_mse: 0.5222\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 735us/step - loss: 0.4780 - mse: 0.4780 - val_loss: 0.4972 - val_mse: 0.4972\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.4596 - mse: 0.4596 - val_loss: 0.4793 - val_mse: 0.4793\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.4461 - mse: 0.4461 - val_loss: 0.4646 - val_mse: 0.4646\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.4352 - mse: 0.4352 - val_loss: 0.4539 - val_mse: 0.4539\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 723us/step - loss: 0.4252 - mse: 0.4252 - val_loss: 0.4443 - val_mse: 0.4443\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 702us/step - loss: 0.4186 - mse: 0.4186 - val_loss: 0.4356 - val_mse: 0.4356\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.4128 - mse: 0.4128 - val_loss: 0.4299 - val_mse: 0.4299\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.4075 - mse: 0.4075 - val_loss: 0.4222 - val_mse: 0.4222\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.4023 - mse: 0.4023 - val_loss: 0.4186 - val_mse: 0.4186\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 717us/step - loss: 0.3985 - mse: 0.3985 - val_loss: 0.4131 - val_mse: 0.4131\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3947 - mse: 0.3947 - val_loss: 0.4140 - val_mse: 0.4140\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.3937 - mse: 0.3937 - val_loss: 0.4054 - val_mse: 0.4054\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 746us/step - loss: 0.3884 - mse: 0.3884 - val_loss: 0.4006 - val_mse: 0.4006\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.3863 - mse: 0.3863 - val_loss: 0.4002 - val_mse: 0.4002\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 735us/step - loss: 0.3834 - mse: 0.3834 - val_loss: 0.3962 - val_mse: 0.3962\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3817 - mse: 0.3817 - val_loss: 0.3936 - val_mse: 0.3936\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 735us/step - loss: 0.3794 - mse: 0.3794 - val_loss: 0.3899 - val_mse: 0.3899\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 738us/step - loss: 0.3773 - mse: 0.3773 - val_loss: 0.3899 - val_mse: 0.3899\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3754 - mse: 0.3754 - val_loss: 0.3875 - val_mse: 0.3875\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.3743 - mse: 0.3743 - val_loss: 0.3865 - val_mse: 0.3865\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 702us/step - loss: 0.3718 - mse: 0.3718 - val_loss: 0.3849 - val_mse: 0.3849\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 725us/step - loss: 0.3700 - mse: 0.3700 - val_loss: 0.3840 - val_mse: 0.3840\n",
      "138/138 [==============================] - 0s 484us/step - loss: 0.3711 - mse: 0.3711\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=   6.5s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 945us/step - loss: 2.0551 - mse: 2.0551 - val_loss: 0.8814 - val_mse: 0.8814\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.6744 - mse: 0.6744 - val_loss: 0.6401 - val_mse: 0.6401\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.5416 - mse: 0.5416 - val_loss: 0.5562 - val_mse: 0.5562\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.4839 - mse: 0.4839 - val_loss: 0.5061 - val_mse: 0.5061\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4500 - mse: 0.4500 - val_loss: 0.4790 - val_mse: 0.4790\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 715us/step - loss: 0.4284 - mse: 0.4284 - val_loss: 0.4549 - val_mse: 0.4549\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 745us/step - loss: 0.4141 - mse: 0.4141 - val_loss: 0.4405 - val_mse: 0.4405\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.4041 - mse: 0.4041 - val_loss: 0.4302 - val_mse: 0.4302\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.3966 - mse: 0.3966 - val_loss: 0.4222 - val_mse: 0.4222\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3905 - mse: 0.3905 - val_loss: 0.4173 - val_mse: 0.4173\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3864 - mse: 0.3864 - val_loss: 0.4109 - val_mse: 0.4109\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.3820 - mse: 0.3820 - val_loss: 0.4058 - val_mse: 0.4058\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.3783 - mse: 0.3783 - val_loss: 0.4044 - val_mse: 0.4044\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 723us/step - loss: 0.3748 - mse: 0.3748 - val_loss: 0.3978 - val_mse: 0.3978\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.3717 - mse: 0.3717 - val_loss: 0.3967 - val_mse: 0.3967\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.3691 - mse: 0.3691 - val_loss: 0.3904 - val_mse: 0.3904\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 821us/step - loss: 0.3661 - mse: 0.3661 - val_loss: 0.3925 - val_mse: 0.3925\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 838us/step - loss: 0.3636 - mse: 0.3636 - val_loss: 0.3865 - val_mse: 0.3865\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 960us/step - loss: 0.3610 - mse: 0.3610 - val_loss: 0.3838 - val_mse: 0.3838\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 858us/step - loss: 0.3593 - mse: 0.3593 - val_loss: 0.3798 - val_mse: 0.3798\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.3560 - mse: 0.3560 - val_loss: 0.3810 - val_mse: 0.3810\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 723us/step - loss: 0.3546 - mse: 0.3546 - val_loss: 0.3787 - val_mse: 0.3787\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.3527 - mse: 0.3527 - val_loss: 0.3749 - val_mse: 0.3749\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 717us/step - loss: 0.3510 - mse: 0.3510 - val_loss: 0.3741 - val_mse: 0.3741\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 720us/step - loss: 0.3494 - mse: 0.3494 - val_loss: 0.3724 - val_mse: 0.3724\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 721us/step - loss: 0.3479 - mse: 0.3479 - val_loss: 0.3688 - val_mse: 0.3688\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 737us/step - loss: 0.3458 - mse: 0.3458 - val_loss: 0.3677 - val_mse: 0.3677\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3441 - mse: 0.3441 - val_loss: 0.3706 - val_mse: 0.3706\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.3432 - mse: 0.3432 - val_loss: 0.3657 - val_mse: 0.3657\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.3410 - mse: 0.3410 - val_loss: 0.3655 - val_mse: 0.3655\n",
      "138/138 [==============================] - 0s 445us/step - loss: 0.3744 - mse: 0.3744\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=   6.6s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 942us/step - loss: 2.7222 - mse: 2.7222 - val_loss: 1.1716 - val_mse: 1.1716\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.8968 - mse: 0.8968 - val_loss: 0.7964 - val_mse: 0.7964\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 783us/step - loss: 0.7068 - mse: 0.7068 - val_loss: 0.6886 - val_mse: 0.6886\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 738us/step - loss: 0.6235 - mse: 0.6235 - val_loss: 0.6229 - val_mse: 0.6229\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.5676 - mse: 0.5676 - val_loss: 0.5740 - val_mse: 0.5740\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 718us/step - loss: 0.5290 - mse: 0.5290 - val_loss: 0.5368 - val_mse: 0.5368\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 696us/step - loss: 0.5008 - mse: 0.5008 - val_loss: 0.5144 - val_mse: 0.5144\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4929 - val_mse: 0.4929\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.4666 - mse: 0.4666 - val_loss: 0.4772 - val_mse: 0.4772\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.4547 - mse: 0.4547 - val_loss: 0.4695 - val_mse: 0.4695\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 715us/step - loss: 0.4456 - mse: 0.4456 - val_loss: 0.4554 - val_mse: 0.4554\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.4371 - mse: 0.4371 - val_loss: 0.4479 - val_mse: 0.4479\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 703us/step - loss: 0.4297 - mse: 0.4297 - val_loss: 0.4441 - val_mse: 0.4441\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.4236 - mse: 0.4236 - val_loss: 0.4333 - val_mse: 0.4333\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 721us/step - loss: 0.4170 - mse: 0.4170 - val_loss: 0.4275 - val_mse: 0.4275\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.4123 - mse: 0.4123 - val_loss: 0.4226 - val_mse: 0.4226\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.4072 - mse: 0.4072 - val_loss: 0.4189 - val_mse: 0.4189\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.4025 - mse: 0.4025 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3992 - mse: 0.3992 - val_loss: 0.4098 - val_mse: 0.4098\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.3955 - mse: 0.3955 - val_loss: 0.4073 - val_mse: 0.4073\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 726us/step - loss: 0.3918 - mse: 0.3918 - val_loss: 0.4032 - val_mse: 0.4032\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 720us/step - loss: 0.3891 - mse: 0.3891 - val_loss: 0.4029 - val_mse: 0.4029\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 725us/step - loss: 0.3865 - mse: 0.3865 - val_loss: 0.3973 - val_mse: 0.3973\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.3838 - mse: 0.3838 - val_loss: 0.3964 - val_mse: 0.3964\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 721us/step - loss: 0.3816 - mse: 0.3816 - val_loss: 0.3942 - val_mse: 0.3942\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 708us/step - loss: 0.3792 - mse: 0.3792 - val_loss: 0.3916 - val_mse: 0.3916\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.3779 - mse: 0.3779 - val_loss: 0.3884 - val_mse: 0.3884\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3757 - mse: 0.3757 - val_loss: 0.3906 - val_mse: 0.3906\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 711us/step - loss: 0.3739 - mse: 0.3739 - val_loss: 0.3863 - val_mse: 0.3863\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 737us/step - loss: 0.3718 - mse: 0.3718 - val_loss: 0.3842 - val_mse: 0.3842\n",
      "138/138 [==============================] - 0s 455us/step - loss: 0.3696 - mse: 0.3696\n",
      "[CV] END learning_rate=0.00037078874137762145, n_hidden=1, n_neurons=51; total time=   6.4s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 985us/step - loss: 0.9293 - mse: 0.9293 - val_loss: 0.4944 - val_mse: 0.4944\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.4178 - mse: 0.4178 - val_loss: 0.4135 - val_mse: 0.4135\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.3820 - mse: 0.3820 - val_loss: 0.3939 - val_mse: 0.3939\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 743us/step - loss: 0.3551 - mse: 0.3551 - val_loss: 0.3520 - val_mse: 0.3520\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 750us/step - loss: 0.3400 - mse: 0.3400 - val_loss: 0.5883 - val_mse: 0.5883\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.3381 - mse: 0.3381 - val_loss: 0.3494 - val_mse: 0.3494\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.3233 - mse: 0.3233 - val_loss: 0.3432 - val_mse: 0.3432\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.3171 - mse: 0.3171 - val_loss: 0.3820 - val_mse: 0.3820\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.3135 - mse: 0.3135 - val_loss: 0.3221 - val_mse: 0.3221\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.3176 - mse: 0.3176 - val_loss: 0.3388 - val_mse: 0.3388\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.3063 - mse: 0.3063 - val_loss: 0.4218 - val_mse: 0.4218\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 761us/step - loss: 0.3044 - mse: 0.3044 - val_loss: 0.3170 - val_mse: 0.3170\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 755us/step - loss: 0.2957 - mse: 0.2957 - val_loss: 0.3178 - val_mse: 0.3178\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2908 - mse: 0.2908 - val_loss: 0.3475 - val_mse: 0.3475\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.2883 - mse: 0.2883 - val_loss: 0.3082 - val_mse: 0.3082\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.2842 - mse: 0.2842 - val_loss: 0.3398 - val_mse: 0.3398\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2831 - mse: 0.2831 - val_loss: 0.3115 - val_mse: 0.3115\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 750us/step - loss: 0.2813 - mse: 0.2813 - val_loss: 0.3116 - val_mse: 0.3116\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 748us/step - loss: 0.2761 - mse: 0.2761 - val_loss: 0.3403 - val_mse: 0.3403\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.3071 - mse: 0.3071 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2793 - mse: 0.2793 - val_loss: 0.3144 - val_mse: 0.3144\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2703 - mse: 0.2703 - val_loss: 0.3093 - val_mse: 0.3093\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 766us/step - loss: 0.2663 - mse: 0.2663 - val_loss: 0.3173 - val_mse: 0.3173\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2670 - mse: 0.2670 - val_loss: 0.3045 - val_mse: 0.3045\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2670 - mse: 0.2670 - val_loss: 0.3064 - val_mse: 0.3064\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2712 - mse: 0.2712 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 747us/step - loss: 0.2635 - mse: 0.2635 - val_loss: 0.3134 - val_mse: 0.3134\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.2662 - mse: 0.2662 - val_loss: 0.3195 - val_mse: 0.3195\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.2655 - mse: 0.2655 - val_loss: 0.3145 - val_mse: 0.3145\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 738us/step - loss: 0.2628 - mse: 0.2628 - val_loss: 0.3261 - val_mse: 0.3261\n",
      "138/138 [==============================] - 0s 471us/step - loss: 0.3082 - mse: 0.3082\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=   6.8s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.6852 - mse: 0.6852 - val_loss: 0.4643 - val_mse: 0.4643\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.3890 - mse: 0.3890 - val_loss: 0.3940 - val_mse: 0.3940\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3596 - mse: 0.3596 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 764us/step - loss: 0.3444 - mse: 0.3444 - val_loss: 0.3499 - val_mse: 0.3499\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.3419 - mse: 0.3419 - val_loss: 0.4869 - val_mse: 0.4869\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.3249 - mse: 0.3249 - val_loss: 0.3621 - val_mse: 0.3621\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.3358 - mse: 0.3358 - val_loss: 0.3586 - val_mse: 0.3586\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.3190 - mse: 0.3190 - val_loss: 0.3251 - val_mse: 0.3251\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.3046 - mse: 0.3046 - val_loss: 0.3232 - val_mse: 0.3232\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.3007 - mse: 0.3007 - val_loss: 0.3354 - val_mse: 0.3354\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2936 - mse: 0.2936 - val_loss: 0.3335 - val_mse: 0.3335\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2920 - mse: 0.2920 - val_loss: 0.3244 - val_mse: 0.3244\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.2880 - mse: 0.2880 - val_loss: 0.3426 - val_mse: 0.3426\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 759us/step - loss: 0.2843 - mse: 0.2843 - val_loss: 0.3205 - val_mse: 0.3205\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2815 - mse: 0.2815 - val_loss: 0.3155 - val_mse: 0.3155\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2785 - mse: 0.2785 - val_loss: 0.3158 - val_mse: 0.3158\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.2774 - mse: 0.2774 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2794 - mse: 0.2794 - val_loss: 0.3321 - val_mse: 0.3321\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.2742 - mse: 0.2742 - val_loss: 1.0194 - val_mse: 1.0194\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.3527 - mse: 0.3527 - val_loss: 0.3265 - val_mse: 0.3265\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.2760 - mse: 0.2760 - val_loss: 0.3333 - val_mse: 0.3333\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2691 - mse: 0.2691 - val_loss: 0.3121 - val_mse: 0.3121\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 797us/step - loss: 0.2676 - mse: 0.2676 - val_loss: 0.3179 - val_mse: 0.3179\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.2651 - mse: 0.2651 - val_loss: 0.3091 - val_mse: 0.3091\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.3169 - val_mse: 0.3169\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 788us/step - loss: 0.2608 - mse: 0.2608 - val_loss: 0.3263 - val_mse: 0.3263\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.2588 - mse: 0.2588 - val_loss: 0.3147 - val_mse: 0.3147\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.2562 - mse: 0.2562 - val_loss: 0.3146 - val_mse: 0.3146\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.2551 - mse: 0.2551 - val_loss: 0.3035 - val_mse: 0.3035\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.2531 - mse: 0.2531 - val_loss: 0.3216 - val_mse: 0.3216\n",
      "138/138 [==============================] - 0s 476us/step - loss: 0.3212 - mse: 0.3212\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=   7.0s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 992us/step - loss: 0.7803 - mse: 0.7803 - val_loss: 0.5076 - val_mse: 0.5076\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.4213 - mse: 0.4213 - val_loss: 0.4824 - val_mse: 0.4824\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.3915 - mse: 0.3915 - val_loss: 0.3816 - val_mse: 0.3816\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 761us/step - loss: 0.3741 - mse: 0.3741 - val_loss: 0.3817 - val_mse: 0.3817\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.3571 - mse: 0.3571 - val_loss: 0.3983 - val_mse: 0.3983\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.3485 - mse: 0.3485 - val_loss: 0.4355 - val_mse: 0.4355\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 761us/step - loss: 0.3390 - mse: 0.3390 - val_loss: 0.3957 - val_mse: 0.3957\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.3313 - mse: 0.3313 - val_loss: 0.3847 - val_mse: 0.3847\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.3232 - mse: 0.3232 - val_loss: 0.3517 - val_mse: 0.3517\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.3157 - mse: 0.3157 - val_loss: 0.3638 - val_mse: 0.3638\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 787us/step - loss: 0.3136 - mse: 0.3136 - val_loss: 0.3593 - val_mse: 0.3593\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 773us/step - loss: 0.3063 - mse: 0.3063 - val_loss: 0.3438 - val_mse: 0.3438\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.3032 - mse: 0.3032 - val_loss: 0.3335 - val_mse: 0.3335\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 797us/step - loss: 0.2981 - mse: 0.2981 - val_loss: 0.3359 - val_mse: 0.3359\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.2932 - mse: 0.2932 - val_loss: 0.3530 - val_mse: 0.3530\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2886 - mse: 0.2886 - val_loss: 0.3406 - val_mse: 0.3406\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 821us/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.3207 - val_mse: 0.3207\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.2840 - mse: 0.2840 - val_loss: 0.3492 - val_mse: 0.3492\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 814us/step - loss: 0.2818 - mse: 0.2818 - val_loss: 0.3297 - val_mse: 0.3297\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 806us/step - loss: 0.2811 - mse: 0.2811 - val_loss: 0.3581 - val_mse: 0.3581\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 820us/step - loss: 0.2749 - mse: 0.2749 - val_loss: 0.3281 - val_mse: 0.3281\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.2750 - mse: 0.2750 - val_loss: 0.3275 - val_mse: 0.3275\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 825us/step - loss: 0.2745 - mse: 0.2745 - val_loss: 0.3242 - val_mse: 0.3242\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 843us/step - loss: 0.2690 - mse: 0.2690 - val_loss: 0.3499 - val_mse: 0.3499\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 848us/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3233 - val_mse: 0.3233\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 817us/step - loss: 0.2666 - mse: 0.2666 - val_loss: 0.3656 - val_mse: 0.3656\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.2636 - mse: 0.2636 - val_loss: 0.3234 - val_mse: 0.3234\n",
      "138/138 [==============================] - 0s 497us/step - loss: 0.3177 - mse: 0.3177\n",
      "[CV] END learning_rate=0.0016535051383872363, n_hidden=2, n_neurons=70; total time=   6.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 973us/step - loss: 0.9399 - mse: 0.9399 - val_loss: 0.5208 - val_mse: 0.5208\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 699us/step - loss: 0.5004 - mse: 0.5004 - val_loss: 0.5017 - val_mse: 0.5017\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.5013 - mse: 0.5013 - val_loss: 0.5197 - val_mse: 0.5197\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 752us/step - loss: 0.4912 - mse: 0.4912 - val_loss: 0.4931 - val_mse: 0.4931\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.4964 - mse: 0.4964 - val_loss: 0.5305 - val_mse: 0.5305\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 694us/step - loss: 0.5018 - mse: 0.5018 - val_loss: 0.5022 - val_mse: 0.5022\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4918 - mse: 0.4918 - val_loss: 0.4997 - val_mse: 0.4997\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 690us/step - loss: 0.4986 - mse: 0.4986 - val_loss: 0.5067 - val_mse: 0.5067\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 662us/step - loss: 0.4951 - mse: 0.4951 - val_loss: 0.4959 - val_mse: 0.4959\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4955 - mse: 0.4955 - val_loss: 0.5029 - val_mse: 0.5029\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 655us/step - loss: 0.4950 - mse: 0.4950 - val_loss: 0.5277 - val_mse: 0.5277\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 658us/step - loss: 0.4941 - mse: 0.4941 - val_loss: 0.4978 - val_mse: 0.4978\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 658us/step - loss: 0.4966 - mse: 0.4966 - val_loss: 0.4957 - val_mse: 0.4957\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 665us/step - loss: 0.4965 - mse: 0.4965 - val_loss: 0.5131 - val_mse: 0.5131\n",
      "138/138 [==============================] - 0s 433us/step - loss: 0.4875 - mse: 0.4875\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   3.0s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 0s 878us/step - loss: 1.1681 - mse: 1.1681 - val_loss: 0.5176 - val_mse: 0.5176\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 667us/step - loss: 0.4938 - mse: 0.4938 - val_loss: 0.5040 - val_mse: 0.5040\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 691us/step - loss: 0.4872 - mse: 0.4872 - val_loss: 0.5037 - val_mse: 0.5037\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 671us/step - loss: 0.4783 - mse: 0.4783 - val_loss: 0.4988 - val_mse: 0.4988\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 671us/step - loss: 0.4816 - mse: 0.4816 - val_loss: 0.5301 - val_mse: 0.5301\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 675us/step - loss: 0.4863 - mse: 0.4863 - val_loss: 0.5036 - val_mse: 0.5036\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 674us/step - loss: 0.4777 - mse: 0.4777 - val_loss: 0.4973 - val_mse: 0.4973\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 734us/step - loss: 0.4827 - mse: 0.4827 - val_loss: 0.4938 - val_mse: 0.4938\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 693us/step - loss: 0.4797 - mse: 0.4797 - val_loss: 0.4997 - val_mse: 0.4997\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4804 - mse: 0.4804 - val_loss: 0.5154 - val_mse: 0.5154\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 658us/step - loss: 0.4832 - mse: 0.4832 - val_loss: 0.5065 - val_mse: 0.5065\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 685us/step - loss: 0.4781 - mse: 0.4781 - val_loss: 0.5015 - val_mse: 0.5015\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4850 - mse: 0.4850 - val_loss: 0.5141 - val_mse: 0.5141\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 679us/step - loss: 0.4801 - mse: 0.4801 - val_loss: 0.4971 - val_mse: 0.4971\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 678us/step - loss: 0.4817 - mse: 0.4817 - val_loss: 0.5056 - val_mse: 0.5056\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4791 - mse: 0.4791 - val_loss: 0.5377 - val_mse: 0.5377\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 667us/step - loss: 0.4845 - mse: 0.4845 - val_loss: 0.4958 - val_mse: 0.4958\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 672us/step - loss: 0.4807 - mse: 0.4807 - val_loss: 0.5014 - val_mse: 0.5014\n",
      "138/138 [==============================] - 0s 462us/step - loss: 0.5014 - mse: 0.5014\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   3.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 0s 868us/step - loss: 0.8673 - mse: 0.8673 - val_loss: 0.5278 - val_mse: 0.5278\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 666us/step - loss: 0.4805 - mse: 0.4805 - val_loss: 0.4982 - val_mse: 0.4982\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 651us/step - loss: 0.4813 - mse: 0.4813 - val_loss: 0.4958 - val_mse: 0.4958\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 665us/step - loss: 0.4818 - mse: 0.4818 - val_loss: 0.5175 - val_mse: 0.5175\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 683us/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.5060 - val_mse: 0.5060\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 665us/step - loss: 0.4831 - mse: 0.4831 - val_loss: 0.4990 - val_mse: 0.4990\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 665us/step - loss: 0.4854 - mse: 0.4854 - val_loss: 0.5036 - val_mse: 0.5036\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 678us/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.4947 - val_mse: 0.4947\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 666us/step - loss: 0.4824 - mse: 0.4824 - val_loss: 0.4990 - val_mse: 0.4990\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 673us/step - loss: 0.4821 - mse: 0.4821 - val_loss: 0.5183 - val_mse: 0.5183\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 661us/step - loss: 0.4841 - mse: 0.4841 - val_loss: 0.5014 - val_mse: 0.5014\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 654us/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.4994 - val_mse: 0.4994\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 660us/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.5019 - val_mse: 0.5019\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 675us/step - loss: 0.4846 - mse: 0.4846 - val_loss: 0.5059 - val_mse: 0.5059\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 684us/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.4905 - val_mse: 0.4905\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 662us/step - loss: 0.4823 - mse: 0.4823 - val_loss: 0.6033 - val_mse: 0.6033\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 669us/step - loss: 0.4851 - mse: 0.4851 - val_loss: 0.4968 - val_mse: 0.4968\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 656us/step - loss: 0.4835 - mse: 0.4835 - val_loss: 0.4984 - val_mse: 0.4984\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 661us/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.4867 - val_mse: 0.4867\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 661us/step - loss: 0.4848 - mse: 0.4848 - val_loss: 0.4944 - val_mse: 0.4944\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 677us/step - loss: 0.4818 - mse: 0.4818 - val_loss: 0.4893 - val_mse: 0.4893\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 648us/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.4977 - val_mse: 0.4977\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 668us/step - loss: 0.4822 - mse: 0.4822 - val_loss: 0.5052 - val_mse: 0.5052\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 670us/step - loss: 0.4829 - mse: 0.4829 - val_loss: 0.4923 - val_mse: 0.4923\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 687us/step - loss: 0.4815 - mse: 0.4815 - val_loss: 0.4957 - val_mse: 0.4957\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 647us/step - loss: 0.4834 - mse: 0.4834 - val_loss: 0.4983 - val_mse: 0.4983\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 666us/step - loss: 0.4837 - mse: 0.4837 - val_loss: 0.4944 - val_mse: 0.4944\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 653us/step - loss: 0.4825 - mse: 0.4825 - val_loss: 0.5017 - val_mse: 0.5017\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 669us/step - loss: 0.4828 - mse: 0.4828 - val_loss: 0.5009 - val_mse: 0.5009\n",
      "138/138 [==============================] - 0s 444us/step - loss: 0.4847 - mse: 0.4847\n",
      "[CV] END learning_rate=0.01824796188192035, n_hidden=0, n_neurons=40; total time=   5.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5770 - mse: 0.5770 - val_loss: 0.4270 - val_mse: 0.4270\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 761us/step - loss: 0.3953 - mse: 0.3953 - val_loss: 0.4475 - val_mse: 0.4475\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.3698 - mse: 0.3698 - val_loss: 0.3726 - val_mse: 0.3726\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.3497 - mse: 0.3497 - val_loss: 0.3487 - val_mse: 0.3487\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.3431 - mse: 0.3431 - val_loss: 0.4855 - val_mse: 0.4855\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 769us/step - loss: 0.3376 - mse: 0.3376 - val_loss: 0.4028 - val_mse: 0.4028\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 764us/step - loss: 0.3267 - mse: 0.3267 - val_loss: 0.3432 - val_mse: 0.3432\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.3205 - mse: 0.3205 - val_loss: 0.3670 - val_mse: 0.3670\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.3093 - mse: 0.3093 - val_loss: 0.3353 - val_mse: 0.3353\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.3058 - mse: 0.3058 - val_loss: 0.3425 - val_mse: 0.3425\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.3037 - mse: 0.3037 - val_loss: 0.4134 - val_mse: 0.4134\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.3011 - mse: 0.3011 - val_loss: 0.3132 - val_mse: 0.3132\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 749us/step - loss: 0.2975 - mse: 0.2975 - val_loss: 0.3208 - val_mse: 0.3208\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.2925 - mse: 0.2925 - val_loss: 0.3278 - val_mse: 0.3278\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 758us/step - loss: 0.2894 - mse: 0.2894 - val_loss: 0.3177 - val_mse: 0.3177\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 752us/step - loss: 0.2940 - mse: 0.2940 - val_loss: 0.3306 - val_mse: 0.3306\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.2952 - val_mse: 0.2952\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.2821 - mse: 0.2821 - val_loss: 0.3069 - val_mse: 0.3069\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.2801 - mse: 0.2801 - val_loss: 0.3078 - val_mse: 0.3078\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 766us/step - loss: 0.2799 - mse: 0.2799 - val_loss: 0.3105 - val_mse: 0.3105\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.2749 - mse: 0.2749 - val_loss: 0.3042 - val_mse: 0.3042\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.2723 - mse: 0.2723 - val_loss: 0.3100 - val_mse: 0.3100\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.2691 - mse: 0.2691 - val_loss: 0.3074 - val_mse: 0.3074\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.2943 - val_mse: 0.2943\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 754us/step - loss: 0.2659 - mse: 0.2659 - val_loss: 0.3098 - val_mse: 0.3098\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2664 - mse: 0.2664 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 794us/step - loss: 0.2646 - mse: 0.2646 - val_loss: 0.3059 - val_mse: 0.3059\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 791us/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.3314 - val_mse: 0.3314\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2627 - mse: 0.2627 - val_loss: 0.3225 - val_mse: 0.3225\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.3155 - val_mse: 0.3155\n",
      "138/138 [==============================] - 0s 530us/step - loss: 0.2972 - mse: 0.2972\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   7.1s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5695 - mse: 0.5695 - val_loss: 0.4658 - val_mse: 0.4658\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.3965 - mse: 0.3965 - val_loss: 0.4560 - val_mse: 0.4560\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 801us/step - loss: 0.3710 - mse: 0.3710 - val_loss: 0.3968 - val_mse: 0.3968\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 796us/step - loss: 0.3566 - mse: 0.3566 - val_loss: 0.3694 - val_mse: 0.3694\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.3504 - mse: 0.3504 - val_loss: 0.4205 - val_mse: 0.4205\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 797us/step - loss: 0.3354 - mse: 0.3354 - val_loss: 0.3793 - val_mse: 0.3793\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.3307 - mse: 0.3307 - val_loss: 0.3635 - val_mse: 0.3635\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.3235 - mse: 0.3235 - val_loss: 0.3497 - val_mse: 0.3497\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 793us/step - loss: 0.3164 - mse: 0.3164 - val_loss: 0.3460 - val_mse: 0.3460\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.3108 - mse: 0.3108 - val_loss: 0.3615 - val_mse: 0.3615\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 794us/step - loss: 0.3090 - mse: 0.3090 - val_loss: 0.3315 - val_mse: 0.3315\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 784us/step - loss: 0.3078 - mse: 0.3078 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.3012 - mse: 0.3012 - val_loss: 0.3445 - val_mse: 0.3445\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 795us/step - loss: 0.2981 - mse: 0.2981 - val_loss: 0.3319 - val_mse: 0.3319\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2908 - mse: 0.2908 - val_loss: 0.3198 - val_mse: 0.3198\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.2883 - mse: 0.2883 - val_loss: 0.3359 - val_mse: 0.3359\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 787us/step - loss: 0.2849 - mse: 0.2849 - val_loss: 0.3261 - val_mse: 0.3261\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.2827 - mse: 0.2827 - val_loss: 0.3340 - val_mse: 0.3340\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2809 - mse: 0.2809 - val_loss: 0.3291 - val_mse: 0.3291\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 776us/step - loss: 0.2876 - mse: 0.2876 - val_loss: 0.3324 - val_mse: 0.3324\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2745 - mse: 0.2745 - val_loss: 0.3257 - val_mse: 0.3257\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2734 - mse: 0.2734 - val_loss: 0.3163 - val_mse: 0.3163\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.2701 - mse: 0.2701 - val_loss: 0.3123 - val_mse: 0.3123\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.2708 - mse: 0.2708 - val_loss: 0.3127 - val_mse: 0.3127\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.2646 - mse: 0.2646 - val_loss: 0.3162 - val_mse: 0.3162\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 764us/step - loss: 0.2709 - mse: 0.2709 - val_loss: 0.3227 - val_mse: 0.3227\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.3119 - val_mse: 0.3119\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.3228 - val_mse: 0.3228\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 784us/step - loss: 0.2605 - mse: 0.2605 - val_loss: 0.3033 - val_mse: 0.3033\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2542 - mse: 0.2542 - val_loss: 0.3106 - val_mse: 0.3106\n",
      "138/138 [==============================] - 0s 466us/step - loss: 0.3174 - mse: 0.3174\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   7.1s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5200 - mse: 0.5200 - val_loss: 0.4699 - val_mse: 0.4699\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.3900 - mse: 0.3900 - val_loss: 0.4106 - val_mse: 0.4106\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.3641 - mse: 0.3641 - val_loss: 0.3786 - val_mse: 0.3786\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.3472 - mse: 0.3472 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 803us/step - loss: 0.3338 - mse: 0.3338 - val_loss: 0.3645 - val_mse: 0.3645\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3247 - mse: 0.3247 - val_loss: 0.3803 - val_mse: 0.3803\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 865us/step - loss: 0.3169 - mse: 0.3169 - val_loss: 0.3806 - val_mse: 0.3806\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 872us/step - loss: 0.3118 - mse: 0.3118 - val_loss: 0.3430 - val_mse: 0.3430\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 758us/step - loss: 0.3031 - mse: 0.3031 - val_loss: 0.3318 - val_mse: 0.3318\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 805us/step - loss: 0.3010 - mse: 0.3010 - val_loss: 0.3385 - val_mse: 0.3385\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 776us/step - loss: 0.2975 - mse: 0.2975 - val_loss: 0.3320 - val_mse: 0.3320\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2935 - mse: 0.2935 - val_loss: 0.3248 - val_mse: 0.3248\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.3167 - val_mse: 0.3167\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.2865 - mse: 0.2865 - val_loss: 0.3259 - val_mse: 0.3259\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.2802 - mse: 0.2802 - val_loss: 0.3152 - val_mse: 0.3152\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 786us/step - loss: 0.2810 - mse: 0.2810 - val_loss: 0.3202 - val_mse: 0.3202\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 777us/step - loss: 0.2805 - mse: 0.2805 - val_loss: 0.3108 - val_mse: 0.3108\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2763 - mse: 0.2763 - val_loss: 0.3168 - val_mse: 0.3168\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 796us/step - loss: 0.2752 - mse: 0.2752 - val_loss: 0.3089 - val_mse: 0.3089\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.2721 - mse: 0.2721 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2683 - mse: 0.2683 - val_loss: 0.3260 - val_mse: 0.3260\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.3235 - val_mse: 0.3235\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 776us/step - loss: 0.2707 - mse: 0.2707 - val_loss: 0.3190 - val_mse: 0.3190\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 809us/step - loss: 0.2679 - mse: 0.2679 - val_loss: 0.3430 - val_mse: 0.3430\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.2645 - mse: 0.2645 - val_loss: 0.3283 - val_mse: 0.3283\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.2629 - mse: 0.2629 - val_loss: 0.3551 - val_mse: 0.3551\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 793us/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.3146 - val_mse: 0.3146\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 764us/step - loss: 0.2578 - mse: 0.2578 - val_loss: 0.3155 - val_mse: 0.3155\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 759us/step - loss: 0.2607 - mse: 0.2607 - val_loss: 0.3282 - val_mse: 0.3282\n",
      "138/138 [==============================] - 0s 473us/step - loss: 0.3190 - mse: 0.3190\n",
      "[CV] END learning_rate=0.0045455096956331, n_hidden=3, n_neurons=30; total time=   7.0s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 939us/step - loss: 1.2706 - mse: 1.2706 - val_loss: 0.5810 - val_mse: 0.5810\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.4984 - mse: 0.4984 - val_loss: 0.4690 - val_mse: 0.4690\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.4341 - mse: 0.4341 - val_loss: 0.4425 - val_mse: 0.4425\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.4106 - mse: 0.4106 - val_loss: 0.4166 - val_mse: 0.4166\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 697us/step - loss: 0.3957 - mse: 0.3957 - val_loss: 0.4237 - val_mse: 0.4237\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 705us/step - loss: 0.3858 - mse: 0.3858 - val_loss: 0.3964 - val_mse: 0.3964\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 722us/step - loss: 0.3762 - mse: 0.3762 - val_loss: 0.3962 - val_mse: 0.3962\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3684 - mse: 0.3684 - val_loss: 0.3859 - val_mse: 0.3859\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3606 - mse: 0.3606 - val_loss: 0.3686 - val_mse: 0.3686\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 729us/step - loss: 0.3558 - mse: 0.3558 - val_loss: 0.3685 - val_mse: 0.3685\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.3517 - mse: 0.3517 - val_loss: 0.4017 - val_mse: 0.4017\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3500 - mse: 0.3500 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.3444 - mse: 0.3444 - val_loss: 0.3551 - val_mse: 0.3551\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 738us/step - loss: 0.3415 - mse: 0.3415 - val_loss: 0.3573 - val_mse: 0.3573\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 734us/step - loss: 0.3392 - mse: 0.3392 - val_loss: 0.3524 - val_mse: 0.3524\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 787us/step - loss: 0.3398 - mse: 0.3398 - val_loss: 0.3520 - val_mse: 0.3520\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 743us/step - loss: 0.3365 - mse: 0.3365 - val_loss: 0.3434 - val_mse: 0.3434\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 728us/step - loss: 0.3357 - mse: 0.3357 - val_loss: 0.3402 - val_mse: 0.3402\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 774us/step - loss: 0.3316 - mse: 0.3316 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 727us/step - loss: 0.3349 - mse: 0.3349 - val_loss: 0.3430 - val_mse: 0.3430\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.3286 - mse: 0.3286 - val_loss: 0.3412 - val_mse: 0.3412\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.3274 - mse: 0.3274 - val_loss: 0.3431 - val_mse: 0.3431\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.3257 - mse: 0.3257 - val_loss: 0.3473 - val_mse: 0.3473\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 692us/step - loss: 0.3257 - mse: 0.3257 - val_loss: 0.3416 - val_mse: 0.3416\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 714us/step - loss: 0.3247 - mse: 0.3247 - val_loss: 0.3384 - val_mse: 0.3384\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3237 - mse: 0.3237 - val_loss: 0.3667 - val_mse: 0.3667\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 719us/step - loss: 0.3212 - mse: 0.3212 - val_loss: 0.3406 - val_mse: 0.3406\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3195 - mse: 0.3195 - val_loss: 0.3359 - val_mse: 0.3359\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3209 - mse: 0.3209 - val_loss: 0.3383 - val_mse: 0.3383\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 738us/step - loss: 0.3179 - mse: 0.3179 - val_loss: 0.3481 - val_mse: 0.3481\n",
      "138/138 [==============================] - 0s 461us/step - loss: 0.3418 - mse: 0.3418\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   6.5s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 930us/step - loss: 1.3428 - mse: 1.3428 - val_loss: 0.5405 - val_mse: 0.5405\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 722us/step - loss: 0.4525 - mse: 0.4525 - val_loss: 0.4713 - val_mse: 0.4713\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 731us/step - loss: 0.4120 - mse: 0.4120 - val_loss: 0.4328 - val_mse: 0.4328\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 713us/step - loss: 0.3947 - mse: 0.3947 - val_loss: 0.4170 - val_mse: 0.4170\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 733us/step - loss: 0.3849 - mse: 0.3849 - val_loss: 0.4527 - val_mse: 0.4527\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 719us/step - loss: 0.3749 - mse: 0.3749 - val_loss: 0.3920 - val_mse: 0.3920\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3681 - mse: 0.3681 - val_loss: 0.3953 - val_mse: 0.3953\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 725us/step - loss: 0.3625 - mse: 0.3625 - val_loss: 0.3800 - val_mse: 0.3800\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 711us/step - loss: 0.3545 - mse: 0.3545 - val_loss: 0.3761 - val_mse: 0.3761\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3495 - mse: 0.3495 - val_loss: 0.3863 - val_mse: 0.3863\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 719us/step - loss: 0.3466 - mse: 0.3466 - val_loss: 0.3718 - val_mse: 0.3718\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 720us/step - loss: 0.3422 - mse: 0.3422 - val_loss: 0.3596 - val_mse: 0.3596\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 715us/step - loss: 0.3397 - mse: 0.3397 - val_loss: 0.3738 - val_mse: 0.3738\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 722us/step - loss: 0.3350 - mse: 0.3350 - val_loss: 0.3547 - val_mse: 0.3547\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3332 - mse: 0.3332 - val_loss: 0.3626 - val_mse: 0.3626\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 719us/step - loss: 0.3377 - mse: 0.3377 - val_loss: 0.3604 - val_mse: 0.3604\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 735us/step - loss: 0.3287 - mse: 0.3287 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 732us/step - loss: 0.3244 - mse: 0.3244 - val_loss: 0.3706 - val_mse: 0.3706\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 711us/step - loss: 0.3231 - mse: 0.3231 - val_loss: 0.4710 - val_mse: 0.4710\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 723us/step - loss: 0.3419 - mse: 0.3419 - val_loss: 0.3507 - val_mse: 0.3507\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3173 - mse: 0.3173 - val_loss: 0.3644 - val_mse: 0.3644\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 711us/step - loss: 0.3172 - mse: 0.3172 - val_loss: 0.3581 - val_mse: 0.3581\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 732us/step - loss: 0.3178 - mse: 0.3178 - val_loss: 0.3450 - val_mse: 0.3450\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.3161 - mse: 0.3161 - val_loss: 0.3546 - val_mse: 0.3546\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3156 - mse: 0.3156 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3141 - mse: 0.3141 - val_loss: 0.3444 - val_mse: 0.3444\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3108 - mse: 0.3108 - val_loss: 0.3491 - val_mse: 0.3491\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 712us/step - loss: 0.3098 - mse: 0.3098 - val_loss: 0.3500 - val_mse: 0.3500\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 724us/step - loss: 0.3099 - mse: 0.3099 - val_loss: 0.3411 - val_mse: 0.3411\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 695us/step - loss: 0.3079 - mse: 0.3079 - val_loss: 0.3445 - val_mse: 0.3445\n",
      "138/138 [==============================] - 0s 459us/step - loss: 0.3403 - mse: 0.3403\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   6.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 915us/step - loss: 1.1188 - mse: 1.1188 - val_loss: 0.5628 - val_mse: 0.5628\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 694us/step - loss: 0.4610 - mse: 0.4610 - val_loss: 0.4489 - val_mse: 0.4489\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.4176 - mse: 0.4176 - val_loss: 0.4259 - val_mse: 0.4259\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.3982 - mse: 0.3982 - val_loss: 0.4092 - val_mse: 0.4092\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 710us/step - loss: 0.3863 - mse: 0.3863 - val_loss: 0.4031 - val_mse: 0.4031\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 699us/step - loss: 0.3762 - mse: 0.3762 - val_loss: 0.3865 - val_mse: 0.3865\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 694us/step - loss: 0.3700 - mse: 0.3700 - val_loss: 0.3898 - val_mse: 0.3898\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.3639 - mse: 0.3639 - val_loss: 0.3738 - val_mse: 0.3738\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 699us/step - loss: 0.3589 - mse: 0.3589 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3545 - mse: 0.3545 - val_loss: 0.3824 - val_mse: 0.3824\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 697us/step - loss: 0.3514 - mse: 0.3514 - val_loss: 0.3629 - val_mse: 0.3629\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 699us/step - loss: 0.3480 - mse: 0.3480 - val_loss: 0.3622 - val_mse: 0.3622\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 694us/step - loss: 0.3457 - mse: 0.3457 - val_loss: 0.3614 - val_mse: 0.3614\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 695us/step - loss: 0.3438 - mse: 0.3438 - val_loss: 0.3627 - val_mse: 0.3627\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3407 - mse: 0.3407 - val_loss: 0.3519 - val_mse: 0.3519\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3386 - mse: 0.3386 - val_loss: 0.3741 - val_mse: 0.3741\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 693us/step - loss: 0.3381 - mse: 0.3381 - val_loss: 0.3509 - val_mse: 0.3509\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 701us/step - loss: 0.3339 - mse: 0.3339 - val_loss: 0.3512 - val_mse: 0.3512\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 688us/step - loss: 0.3328 - mse: 0.3328 - val_loss: 0.3415 - val_mse: 0.3415\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 697us/step - loss: 0.3310 - mse: 0.3310 - val_loss: 0.3459 - val_mse: 0.3459\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 716us/step - loss: 0.3278 - mse: 0.3278 - val_loss: 0.3382 - val_mse: 0.3382\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 691us/step - loss: 0.3264 - mse: 0.3264 - val_loss: 0.3499 - val_mse: 0.3499\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 684us/step - loss: 0.3262 - mse: 0.3262 - val_loss: 0.3387 - val_mse: 0.3387\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3227 - mse: 0.3227 - val_loss: 0.3633 - val_mse: 0.3633\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 691us/step - loss: 0.3229 - mse: 0.3229 - val_loss: 0.3417 - val_mse: 0.3417\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 698us/step - loss: 0.3201 - mse: 0.3201 - val_loss: 0.3535 - val_mse: 0.3535\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 709us/step - loss: 0.3185 - mse: 0.3185 - val_loss: 0.3368 - val_mse: 0.3368\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 687us/step - loss: 0.3163 - mse: 0.3163 - val_loss: 0.3380 - val_mse: 0.3380\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 690us/step - loss: 0.3168 - mse: 0.3168 - val_loss: 0.3387 - val_mse: 0.3387\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 706us/step - loss: 0.3145 - mse: 0.3145 - val_loss: 0.3362 - val_mse: 0.3362\n",
      "138/138 [==============================] - 0s 443us/step - loss: 0.3279 - mse: 0.3279\n",
      "[CV] END learning_rate=0.0020587676114196545, n_hidden=1, n_neurons=49; total time=   6.3s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.6027 - mse: 0.6027 - val_loss: 0.4927 - val_mse: 0.4927\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 836us/step - loss: 0.3975 - mse: 0.3975 - val_loss: 0.3787 - val_mse: 0.3787\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 811us/step - loss: 0.3666 - mse: 0.3666 - val_loss: 0.3669 - val_mse: 0.3669\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 810us/step - loss: 0.3418 - mse: 0.3418 - val_loss: 0.3364 - val_mse: 0.3364\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 852us/step - loss: 0.3367 - mse: 0.3367 - val_loss: 0.4365 - val_mse: 0.4365\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.3289 - mse: 0.3289 - val_loss: 0.3887 - val_mse: 0.3887\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 839us/step - loss: 0.3206 - mse: 0.3206 - val_loss: 0.3376 - val_mse: 0.3376\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.3099 - mse: 0.3099 - val_loss: 0.3921 - val_mse: 0.3921\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 817us/step - loss: 0.3068 - mse: 0.3068 - val_loss: 0.3126 - val_mse: 0.3126\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 816us/step - loss: 0.3001 - mse: 0.3001 - val_loss: 0.3245 - val_mse: 0.3245\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 818us/step - loss: 0.2996 - mse: 0.2996 - val_loss: 0.4660 - val_mse: 0.4660\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 813us/step - loss: 0.2993 - mse: 0.2993 - val_loss: 0.3201 - val_mse: 0.3201\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 854us/step - loss: 0.2893 - mse: 0.2893 - val_loss: 0.3121 - val_mse: 0.3121\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 809us/step - loss: 0.2859 - mse: 0.2859 - val_loss: 0.3711 - val_mse: 0.3711\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 816us/step - loss: 0.2836 - mse: 0.2836 - val_loss: 0.3069 - val_mse: 0.3069\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 802us/step - loss: 0.2891 - mse: 0.2891 - val_loss: 0.3241 - val_mse: 0.3241\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 801us/step - loss: 0.2825 - mse: 0.2825 - val_loss: 0.2953 - val_mse: 0.2953\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.2739 - mse: 0.2739 - val_loss: 0.3255 - val_mse: 0.3255\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 807us/step - loss: 0.2722 - mse: 0.2722 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 812us/step - loss: 0.2718 - mse: 0.2718 - val_loss: 0.3179 - val_mse: 0.3179\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 827us/step - loss: 0.2669 - mse: 0.2669 - val_loss: 0.3154 - val_mse: 0.3154\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 809us/step - loss: 0.2678 - mse: 0.2678 - val_loss: 0.3069 - val_mse: 0.3069\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 828us/step - loss: 0.2595 - mse: 0.2595 - val_loss: 0.3142 - val_mse: 0.3142\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.2605 - mse: 0.2605 - val_loss: 0.2981 - val_mse: 0.2981\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 805us/step - loss: 0.2560 - mse: 0.2560 - val_loss: 0.2990 - val_mse: 0.2990\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 824us/step - loss: 0.2560 - mse: 0.2560 - val_loss: 0.3990 - val_mse: 0.3990\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.2493 - mse: 0.2493 - val_loss: 0.3207 - val_mse: 0.3207\n",
      "138/138 [==============================] - 0s 448us/step - loss: 0.3048 - mse: 0.3048\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   6.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5677 - mse: 0.5677 - val_loss: 0.4763 - val_mse: 0.4763\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3893 - mse: 0.3893 - val_loss: 0.3913 - val_mse: 0.3913\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3544 - mse: 0.3544 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 812us/step - loss: 0.3387 - mse: 0.3387 - val_loss: 0.3443 - val_mse: 0.3443\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 798us/step - loss: 0.3265 - mse: 0.3265 - val_loss: 0.4524 - val_mse: 0.4524\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 816us/step - loss: 0.3162 - mse: 0.3162 - val_loss: 0.3761 - val_mse: 0.3761\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 800us/step - loss: 0.3062 - mse: 0.3062 - val_loss: 0.3533 - val_mse: 0.3533\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 824us/step - loss: 0.3017 - mse: 0.3017 - val_loss: 0.3550 - val_mse: 0.3550\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2912 - mse: 0.2912 - val_loss: 0.3339 - val_mse: 0.3339\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 814us/step - loss: 0.2918 - mse: 0.2918 - val_loss: 0.3517 - val_mse: 0.3517\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 814us/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.3187 - val_mse: 0.3187\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 848us/step - loss: 0.2824 - mse: 0.2824 - val_loss: 0.3062 - val_mse: 0.3062\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.2777 - mse: 0.2777 - val_loss: 0.3292 - val_mse: 0.3292\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 890us/step - loss: 0.2772 - mse: 0.2772 - val_loss: 0.3216 - val_mse: 0.3216\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 818us/step - loss: 0.2712 - mse: 0.2712 - val_loss: 0.3018 - val_mse: 0.3018\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2663 - mse: 0.2663 - val_loss: 0.3081 - val_mse: 0.3081\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 845us/step - loss: 0.2613 - mse: 0.2613 - val_loss: 0.3294 - val_mse: 0.3294\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 809us/step - loss: 0.2626 - mse: 0.2626 - val_loss: 0.3248 - val_mse: 0.3248\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 840us/step - loss: 0.2623 - mse: 0.2623 - val_loss: 0.3149 - val_mse: 0.3149\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 806us/step - loss: 0.2565 - mse: 0.2565 - val_loss: 0.3424 - val_mse: 0.3424\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 834us/step - loss: 0.2523 - mse: 0.2523 - val_loss: 0.3246 - val_mse: 0.3246\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 805us/step - loss: 0.2534 - mse: 0.2534 - val_loss: 0.3075 - val_mse: 0.3075\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 805us/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.2991 - val_mse: 0.2991\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.3128 - val_mse: 0.3128\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 802us/step - loss: 0.2452 - mse: 0.2452 - val_loss: 0.3187 - val_mse: 0.3187\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2470 - mse: 0.2470 - val_loss: 0.3490 - val_mse: 0.3490\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 814us/step - loss: 0.2466 - mse: 0.2466 - val_loss: 0.3046 - val_mse: 0.3046\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 802us/step - loss: 0.2382 - mse: 0.2382 - val_loss: 0.3094 - val_mse: 0.3094\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.2375 - mse: 0.2375 - val_loss: 0.3107 - val_mse: 0.3107\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 823us/step - loss: 0.2402 - mse: 0.2402 - val_loss: 0.3149 - val_mse: 0.3149\n",
      "138/138 [==============================] - 0s 478us/step - loss: 0.3059 - mse: 0.3059\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   7.4s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5362 - mse: 0.5362 - val_loss: 0.4316 - val_mse: 0.4316\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 818us/step - loss: 0.3849 - mse: 0.3849 - val_loss: 0.3672 - val_mse: 0.3672\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.3568 - mse: 0.3568 - val_loss: 0.3572 - val_mse: 0.3572\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 803us/step - loss: 0.3400 - mse: 0.3400 - val_loss: 0.3319 - val_mse: 0.3319\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 821us/step - loss: 0.3262 - mse: 0.3262 - val_loss: 0.3444 - val_mse: 0.3444\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 801us/step - loss: 0.3185 - mse: 0.3185 - val_loss: 0.4144 - val_mse: 0.4144\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.3121 - mse: 0.3121 - val_loss: 0.4472 - val_mse: 0.4472\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 788us/step - loss: 0.3059 - mse: 0.3059 - val_loss: 0.3212 - val_mse: 0.3212\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2949 - mse: 0.2949 - val_loss: 0.3528 - val_mse: 0.3528\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 818us/step - loss: 0.2922 - mse: 0.2922 - val_loss: 0.3281 - val_mse: 0.3281\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 829us/step - loss: 0.2935 - mse: 0.2935 - val_loss: 0.3120 - val_mse: 0.3120\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 830us/step - loss: 0.2843 - mse: 0.2843 - val_loss: 0.3365 - val_mse: 0.3365\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 796us/step - loss: 0.2826 - mse: 0.2826 - val_loss: 0.3374 - val_mse: 0.3374\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 818us/step - loss: 0.2792 - mse: 0.2792 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 844us/step - loss: 0.2729 - mse: 0.2729 - val_loss: 0.3246 - val_mse: 0.3246\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 810us/step - loss: 0.2680 - mse: 0.2680 - val_loss: 0.3410 - val_mse: 0.3410\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2704 - mse: 0.2704 - val_loss: 0.2975 - val_mse: 0.2975\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.2671 - mse: 0.2671 - val_loss: 0.3152 - val_mse: 0.3152\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 791us/step - loss: 0.2637 - mse: 0.2637 - val_loss: 0.3165 - val_mse: 0.3165\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 823us/step - loss: 0.2618 - mse: 0.2618 - val_loss: 0.3679 - val_mse: 0.3679\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 813us/step - loss: 0.2559 - mse: 0.2559 - val_loss: 0.3273 - val_mse: 0.3273\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.2553 - mse: 0.2553 - val_loss: 0.3174 - val_mse: 0.3174\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.2568 - mse: 0.2568 - val_loss: 0.3135 - val_mse: 0.3135\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 796us/step - loss: 0.2544 - mse: 0.2544 - val_loss: 0.3178 - val_mse: 0.3178\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.2541 - mse: 0.2541 - val_loss: 0.3309 - val_mse: 0.3309\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 836us/step - loss: 0.2520 - mse: 0.2520 - val_loss: 0.3426 - val_mse: 0.3426\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 812us/step - loss: 0.2468 - mse: 0.2468 - val_loss: 0.3126 - val_mse: 0.3126\n",
      "138/138 [==============================] - 0s 468us/step - loss: 0.3183 - mse: 0.3183\n",
      "[CV] END learning_rate=0.005803602934201024, n_hidden=3, n_neurons=74; total time=   6.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5487 - mse: 0.5487 - val_loss: 0.5403 - val_mse: 0.5403\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.3859 - mse: 0.3859 - val_loss: 0.3733 - val_mse: 0.3733\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 824us/step - loss: 0.3637 - mse: 0.3637 - val_loss: 0.3527 - val_mse: 0.3527\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 811us/step - loss: 0.3434 - mse: 0.3434 - val_loss: 0.3340 - val_mse: 0.3340\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.3331 - mse: 0.3331 - val_loss: 0.4664 - val_mse: 0.4664\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 829us/step - loss: 0.3267 - mse: 0.3267 - val_loss: 0.3863 - val_mse: 0.3863\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 808us/step - loss: 0.3257 - mse: 0.3257 - val_loss: 0.3389 - val_mse: 0.3389\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 813us/step - loss: 0.3120 - mse: 0.3120 - val_loss: 0.4041 - val_mse: 0.4041\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 834us/step - loss: 0.3022 - mse: 0.3022 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 811us/step - loss: 0.3001 - mse: 0.3001 - val_loss: 0.3404 - val_mse: 0.3404\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 827us/step - loss: 0.2993 - mse: 0.2993 - val_loss: 0.4299 - val_mse: 0.4299\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 810us/step - loss: 0.2956 - mse: 0.2956 - val_loss: 0.3110 - val_mse: 0.3110\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.2904 - mse: 0.2904 - val_loss: 0.3132 - val_mse: 0.3132\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 821us/step - loss: 0.2849 - mse: 0.2849 - val_loss: 0.3482 - val_mse: 0.3482\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.2809 - mse: 0.2809 - val_loss: 0.3216 - val_mse: 0.3216\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 811us/step - loss: 0.2840 - mse: 0.2840 - val_loss: 0.3416 - val_mse: 0.3416\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 822us/step - loss: 0.2783 - mse: 0.2783 - val_loss: 0.3028 - val_mse: 0.3028\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 806us/step - loss: 0.2773 - mse: 0.2773 - val_loss: 0.3361 - val_mse: 0.3361\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2736 - mse: 0.2736 - val_loss: 0.3039 - val_mse: 0.3039\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2750 - mse: 0.2750 - val_loss: 0.3118 - val_mse: 0.3118\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2678 - mse: 0.2678 - val_loss: 0.3161 - val_mse: 0.3161\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 843us/step - loss: 0.2643 - mse: 0.2643 - val_loss: 0.3074 - val_mse: 0.3074\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 831us/step - loss: 0.2622 - mse: 0.2622 - val_loss: 0.3301 - val_mse: 0.3301\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 844us/step - loss: 0.2633 - mse: 0.2633 - val_loss: 0.3041 - val_mse: 0.3041\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 817us/step - loss: 0.2596 - mse: 0.2596 - val_loss: 0.3069 - val_mse: 0.3069\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 820us/step - loss: 0.2580 - mse: 0.2580 - val_loss: 0.3518 - val_mse: 0.3518\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 817us/step - loss: 0.2578 - mse: 0.2578 - val_loss: 0.3238 - val_mse: 0.3238\n",
      "138/138 [==============================] - 0s 474us/step - loss: 0.3044 - mse: 0.3044\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   6.8s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5745 - mse: 0.5745 - val_loss: 0.4511 - val_mse: 0.4511\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 902us/step - loss: 0.4036 - mse: 0.4036 - val_loss: 0.3923 - val_mse: 0.3923\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 870us/step - loss: 0.3620 - mse: 0.3620 - val_loss: 0.4259 - val_mse: 0.4259\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 849us/step - loss: 0.3465 - mse: 0.3465 - val_loss: 0.3570 - val_mse: 0.3570\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 810us/step - loss: 0.3361 - mse: 0.3361 - val_loss: 0.6513 - val_mse: 0.6513\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 896us/step - loss: 0.3301 - mse: 0.3301 - val_loss: 0.4303 - val_mse: 0.4303\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 858us/step - loss: 0.3170 - mse: 0.3170 - val_loss: 0.4117 - val_mse: 0.4117\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 825us/step - loss: 0.3102 - mse: 0.3102 - val_loss: 0.3649 - val_mse: 0.3649\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2974 - mse: 0.2974 - val_loss: 0.3271 - val_mse: 0.3271\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 828us/step - loss: 0.2962 - mse: 0.2962 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2896 - mse: 0.2896 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 868us/step - loss: 0.2880 - mse: 0.2880 - val_loss: 0.3062 - val_mse: 0.3062\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 837us/step - loss: 0.2796 - mse: 0.2796 - val_loss: 0.3122 - val_mse: 0.3122\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 849us/step - loss: 0.2754 - mse: 0.2754 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 866us/step - loss: 0.2738 - mse: 0.2738 - val_loss: 0.2959 - val_mse: 0.2959\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.2694 - mse: 0.2694 - val_loss: 0.3020 - val_mse: 0.3020\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 854us/step - loss: 0.2648 - mse: 0.2648 - val_loss: 0.3197 - val_mse: 0.3197\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 835us/step - loss: 0.2674 - mse: 0.2674 - val_loss: 0.3214 - val_mse: 0.3214\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 887us/step - loss: 0.2631 - mse: 0.2631 - val_loss: 0.3094 - val_mse: 0.3094\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 848us/step - loss: 0.2578 - mse: 0.2578 - val_loss: 0.3182 - val_mse: 0.3182\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 829us/step - loss: 0.2557 - mse: 0.2557 - val_loss: 0.3264 - val_mse: 0.3264\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 847us/step - loss: 0.2519 - mse: 0.2519 - val_loss: 0.3046 - val_mse: 0.3046\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 862us/step - loss: 0.2545 - mse: 0.2545 - val_loss: 0.3101 - val_mse: 0.3101\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 875us/step - loss: 0.2525 - mse: 0.2525 - val_loss: 0.3166 - val_mse: 0.3166\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.2440 - mse: 0.2440 - val_loss: 0.3118 - val_mse: 0.3118\n",
      "138/138 [==============================] - 0s 489us/step - loss: 0.3078 - mse: 0.3078\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   6.5s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5639 - mse: 0.5639 - val_loss: 0.4077 - val_mse: 0.4077\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 817us/step - loss: 0.3797 - mse: 0.3797 - val_loss: 0.4235 - val_mse: 0.4235\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 846us/step - loss: 0.3588 - mse: 0.3588 - val_loss: 0.3763 - val_mse: 0.3763\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 830us/step - loss: 0.3345 - mse: 0.3345 - val_loss: 0.3441 - val_mse: 0.3441\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 833us/step - loss: 0.3266 - mse: 0.3266 - val_loss: 0.3399 - val_mse: 0.3399\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 847us/step - loss: 0.3210 - mse: 0.3210 - val_loss: 0.3845 - val_mse: 0.3845\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 843us/step - loss: 0.3158 - mse: 0.3158 - val_loss: 0.4330 - val_mse: 0.4330\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 844us/step - loss: 0.3058 - mse: 0.3058 - val_loss: 0.3283 - val_mse: 0.3283\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 836us/step - loss: 0.2982 - mse: 0.2982 - val_loss: 0.3400 - val_mse: 0.3400\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 862us/step - loss: 0.2949 - mse: 0.2949 - val_loss: 0.3320 - val_mse: 0.3320\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 862us/step - loss: 0.2989 - mse: 0.2989 - val_loss: 0.3112 - val_mse: 0.3112\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 822us/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.3487 - val_mse: 0.3487\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 844us/step - loss: 0.2851 - mse: 0.2851 - val_loss: 0.3551 - val_mse: 0.3551\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 848us/step - loss: 0.2888 - mse: 0.2888 - val_loss: 0.3282 - val_mse: 0.3282\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 874us/step - loss: 0.2758 - mse: 0.2758 - val_loss: 0.3411 - val_mse: 0.3411\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 875us/step - loss: 0.2791 - mse: 0.2791 - val_loss: 0.3303 - val_mse: 0.3303\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 852us/step - loss: 0.2807 - mse: 0.2807 - val_loss: 0.3054 - val_mse: 0.3054\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 0.2708 - mse: 0.2708 - val_loss: 0.3256 - val_mse: 0.3256\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 956us/step - loss: 0.2704 - mse: 0.2704 - val_loss: 0.3298 - val_mse: 0.3298\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 923us/step - loss: 0.2673 - mse: 0.2673 - val_loss: 0.3698 - val_mse: 0.3698\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 874us/step - loss: 0.2650 - mse: 0.2650 - val_loss: 0.3183 - val_mse: 0.3183\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 0.2624 - mse: 0.2624 - val_loss: 0.3152 - val_mse: 0.3152\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 0.2619 - mse: 0.2619 - val_loss: 0.3409 - val_mse: 0.3409\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 938us/step - loss: 0.2606 - mse: 0.2606 - val_loss: 0.3180 - val_mse: 0.3180\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 906us/step - loss: 0.2585 - mse: 0.2585 - val_loss: 0.3658 - val_mse: 0.3658\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 887us/step - loss: 0.2579 - mse: 0.2579 - val_loss: 0.3631 - val_mse: 0.3631\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 1ms/step - loss: 0.2506 - mse: 0.2506 - val_loss: 0.3365 - val_mse: 0.3365\n",
      "138/138 [==============================] - 0s 598us/step - loss: 0.3150 - mse: 0.3150\n",
      "[CV] END learning_rate=0.0059640580092043885, n_hidden=3, n_neurons=80; total time=   7.7s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5971 - mse: 0.5971 - val_loss: 0.4574 - val_mse: 0.4574\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 803us/step - loss: 0.3919 - mse: 0.3919 - val_loss: 0.4159 - val_mse: 0.4159\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 804us/step - loss: 0.3667 - mse: 0.3667 - val_loss: 0.3713 - val_mse: 0.3713\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 783us/step - loss: 0.3467 - mse: 0.3467 - val_loss: 0.3501 - val_mse: 0.3501\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.3419 - mse: 0.3419 - val_loss: 0.5782 - val_mse: 0.5782\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 898us/step - loss: 0.3318 - mse: 0.3318 - val_loss: 0.3860 - val_mse: 0.3860\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 814us/step - loss: 0.3215 - mse: 0.3215 - val_loss: 0.3429 - val_mse: 0.3429\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 803us/step - loss: 0.3154 - mse: 0.3154 - val_loss: 0.3841 - val_mse: 0.3841\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 815us/step - loss: 0.3122 - mse: 0.3122 - val_loss: 0.3421 - val_mse: 0.3421\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 767us/step - loss: 0.3016 - mse: 0.3016 - val_loss: 0.3466 - val_mse: 0.3466\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 795us/step - loss: 0.2993 - mse: 0.2993 - val_loss: 0.4705 - val_mse: 0.4705\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 776us/step - loss: 0.2991 - mse: 0.2991 - val_loss: 0.3100 - val_mse: 0.3100\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.2909 - mse: 0.2909 - val_loss: 0.3272 - val_mse: 0.3272\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 789us/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.3511 - val_mse: 0.3511\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.2850 - mse: 0.2850 - val_loss: 0.3194 - val_mse: 0.3194\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 754us/step - loss: 0.2840 - mse: 0.2840 - val_loss: 0.3424 - val_mse: 0.3424\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 795us/step - loss: 0.2800 - mse: 0.2800 - val_loss: 0.3093 - val_mse: 0.3093\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.2753 - mse: 0.2753 - val_loss: 0.3112 - val_mse: 0.3112\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2740 - mse: 0.2740 - val_loss: 0.3328 - val_mse: 0.3328\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 781us/step - loss: 0.3073 - mse: 0.3073 - val_loss: 0.3324 - val_mse: 0.3324\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.2749 - mse: 0.2749 - val_loss: 0.3150 - val_mse: 0.3150\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2692 - mse: 0.2692 - val_loss: 0.3159 - val_mse: 0.3159\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.2686 - mse: 0.2686 - val_loss: 0.3191 - val_mse: 0.3191\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.2662 - mse: 0.2662 - val_loss: 0.3091 - val_mse: 0.3091\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 779us/step - loss: 0.2654 - mse: 0.2654 - val_loss: 0.3120 - val_mse: 0.3120\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2647 - mse: 0.2647 - val_loss: 0.3850 - val_mse: 0.3850\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 762us/step - loss: 0.2614 - mse: 0.2614 - val_loss: 0.3111 - val_mse: 0.3111\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 826us/step - loss: 0.2577 - mse: 0.2577 - val_loss: 0.3228 - val_mse: 0.3228\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 766us/step - loss: 0.2589 - mse: 0.2589 - val_loss: 0.3224 - val_mse: 0.3224\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 755us/step - loss: 0.2536 - mse: 0.2536 - val_loss: 0.3275 - val_mse: 0.3275\n",
      "138/138 [==============================] - 0s 496us/step - loss: 0.3023 - mse: 0.3023\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   7.2s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.5988 - mse: 0.5988 - val_loss: 0.4834 - val_mse: 0.4834\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 740us/step - loss: 0.3854 - mse: 0.3854 - val_loss: 0.3955 - val_mse: 0.3955\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.3641 - mse: 0.3641 - val_loss: 0.3812 - val_mse: 0.3812\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.3713 - mse: 0.3713 - val_loss: 0.3699 - val_mse: 0.3699\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 765us/step - loss: 0.3333 - mse: 0.3333 - val_loss: 0.4948 - val_mse: 0.4948\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 773us/step - loss: 0.3250 - mse: 0.3250 - val_loss: 0.3647 - val_mse: 0.3647\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 744us/step - loss: 0.3156 - mse: 0.3156 - val_loss: 0.3785 - val_mse: 0.3785\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 749us/step - loss: 0.3110 - mse: 0.3110 - val_loss: 0.3498 - val_mse: 0.3498\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.3071 - mse: 0.3071 - val_loss: 0.3384 - val_mse: 0.3384\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 745us/step - loss: 0.3003 - mse: 0.3003 - val_loss: 0.3358 - val_mse: 0.3358\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 729us/step - loss: 0.2921 - mse: 0.2921 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2907 - mse: 0.2907 - val_loss: 0.3071 - val_mse: 0.3071\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 751us/step - loss: 0.2849 - mse: 0.2849 - val_loss: 0.3354 - val_mse: 0.3354\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 730us/step - loss: 0.2864 - mse: 0.2864 - val_loss: 0.3184 - val_mse: 0.3184\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 754us/step - loss: 0.2804 - mse: 0.2804 - val_loss: 0.3132 - val_mse: 0.3132\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 757us/step - loss: 0.2841 - mse: 0.2841 - val_loss: 0.3130 - val_mse: 0.3130\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 726us/step - loss: 0.2769 - mse: 0.2769 - val_loss: 0.3200 - val_mse: 0.3200\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2736 - mse: 0.2736 - val_loss: 0.3325 - val_mse: 0.3325\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 732us/step - loss: 0.2704 - mse: 0.2704 - val_loss: 0.4130 - val_mse: 0.4130\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 748us/step - loss: 0.2917 - mse: 0.2917 - val_loss: 0.3215 - val_mse: 0.3215\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.2659 - mse: 0.2659 - val_loss: 0.3341 - val_mse: 0.3341\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 732us/step - loss: 0.2654 - mse: 0.2654 - val_loss: 0.3125 - val_mse: 0.3125\n",
      "138/138 [==============================] - 0s 474us/step - loss: 0.3226 - mse: 0.3226\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   5.1s\n",
      "Epoch 1/30\n",
      "276/276 [==============================] - 1s 1ms/step - loss: 0.6323 - mse: 0.6323 - val_loss: 0.4063 - val_mse: 0.4063\n",
      "Epoch 2/30\n",
      "276/276 [==============================] - 0s 756us/step - loss: 0.3780 - mse: 0.3780 - val_loss: 0.3841 - val_mse: 0.3841\n",
      "Epoch 3/30\n",
      "276/276 [==============================] - 0s 768us/step - loss: 0.3544 - mse: 0.3544 - val_loss: 0.3649 - val_mse: 0.3649\n",
      "Epoch 4/30\n",
      "276/276 [==============================] - 0s 771us/step - loss: 0.3381 - mse: 0.3381 - val_loss: 0.3574 - val_mse: 0.3574\n",
      "Epoch 5/30\n",
      "276/276 [==============================] - 0s 760us/step - loss: 0.3234 - mse: 0.3234 - val_loss: 0.3376 - val_mse: 0.3376\n",
      "Epoch 6/30\n",
      "276/276 [==============================] - 0s 753us/step - loss: 0.3184 - mse: 0.3184 - val_loss: 0.4155 - val_mse: 0.4155\n",
      "Epoch 7/30\n",
      "276/276 [==============================] - 0s 819us/step - loss: 0.3112 - mse: 0.3112 - val_loss: 0.3948 - val_mse: 0.3948\n",
      "Epoch 8/30\n",
      "276/276 [==============================] - 0s 806us/step - loss: 0.3078 - mse: 0.3078 - val_loss: 0.3314 - val_mse: 0.3314\n",
      "Epoch 9/30\n",
      "276/276 [==============================] - 0s 834us/step - loss: 0.2967 - mse: 0.2967 - val_loss: 0.3378 - val_mse: 0.3378\n",
      "Epoch 10/30\n",
      "276/276 [==============================] - 0s 784us/step - loss: 0.2955 - mse: 0.2955 - val_loss: 0.3268 - val_mse: 0.3268\n",
      "Epoch 11/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.2955 - mse: 0.2955 - val_loss: 0.3259 - val_mse: 0.3259\n",
      "Epoch 12/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.2886 - mse: 0.2886 - val_loss: 0.3345 - val_mse: 0.3345\n",
      "Epoch 13/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2833 - mse: 0.2833 - val_loss: 0.3197 - val_mse: 0.3197\n",
      "Epoch 14/30\n",
      "276/276 [==============================] - 0s 778us/step - loss: 0.2825 - mse: 0.2825 - val_loss: 0.3277 - val_mse: 0.3277\n",
      "Epoch 15/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2773 - mse: 0.2773 - val_loss: 0.3292 - val_mse: 0.3292\n",
      "Epoch 16/30\n",
      "276/276 [==============================] - 0s 810us/step - loss: 0.2738 - mse: 0.2738 - val_loss: 0.3283 - val_mse: 0.3283\n",
      "Epoch 17/30\n",
      "276/276 [==============================] - 0s 823us/step - loss: 0.2770 - mse: 0.2770 - val_loss: 0.2974 - val_mse: 0.2974\n",
      "Epoch 18/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.2715 - mse: 0.2715 - val_loss: 0.3180 - val_mse: 0.3180\n",
      "Epoch 19/30\n",
      "276/276 [==============================] - 0s 766us/step - loss: 0.2717 - mse: 0.2717 - val_loss: 0.3123 - val_mse: 0.3123\n",
      "Epoch 20/30\n",
      "276/276 [==============================] - 0s 782us/step - loss: 0.2695 - mse: 0.2695 - val_loss: 0.3403 - val_mse: 0.3403\n",
      "Epoch 21/30\n",
      "276/276 [==============================] - 0s 803us/step - loss: 0.2645 - mse: 0.2645 - val_loss: 0.3084 - val_mse: 0.3084\n",
      "Epoch 22/30\n",
      "276/276 [==============================] - 0s 775us/step - loss: 0.2642 - mse: 0.2642 - val_loss: 0.3097 - val_mse: 0.3097\n",
      "Epoch 23/30\n",
      "276/276 [==============================] - 0s 794us/step - loss: 0.2653 - mse: 0.2653 - val_loss: 0.2999 - val_mse: 0.2999\n",
      "Epoch 24/30\n",
      "276/276 [==============================] - 0s 795us/step - loss: 0.2614 - mse: 0.2614 - val_loss: 0.3049 - val_mse: 0.3049\n",
      "Epoch 25/30\n",
      "276/276 [==============================] - 0s 785us/step - loss: 0.2603 - mse: 0.2603 - val_loss: 0.3244 - val_mse: 0.3244\n",
      "Epoch 26/30\n",
      "276/276 [==============================] - 0s 790us/step - loss: 0.2588 - mse: 0.2588 - val_loss: 0.3356 - val_mse: 0.3356\n",
      "Epoch 27/30\n",
      "276/276 [==============================] - 0s 772us/step - loss: 0.2549 - mse: 0.2549 - val_loss: 0.2948 - val_mse: 0.2948\n",
      "Epoch 28/30\n",
      "276/276 [==============================] - 0s 792us/step - loss: 0.2515 - mse: 0.2515 - val_loss: 0.3037 - val_mse: 0.3037\n",
      "Epoch 29/30\n",
      "276/276 [==============================] - 0s 763us/step - loss: 0.2542 - mse: 0.2542 - val_loss: 0.3197 - val_mse: 0.3197\n",
      "Epoch 30/30\n",
      "276/276 [==============================] - 0s 770us/step - loss: 0.2498 - mse: 0.2498 - val_loss: 0.3208 - val_mse: 0.3208\n",
      "138/138 [==============================] - 0s 463us/step - loss: 0.3074 - mse: 0.3074\n",
      "[CV] END learning_rate=0.004591455636549438, n_hidden=2, n_neurons=59; total time=   7.1s\n",
      "Epoch 1/30\n",
      "413/413 [==============================] - 1s 928us/step - loss: 0.5448 - mse: 0.5448 - val_loss: 0.4786 - val_mse: 0.4786\n",
      "Epoch 2/30\n",
      "413/413 [==============================] - 0s 763us/step - loss: 0.3624 - mse: 0.3624 - val_loss: 0.3455 - val_mse: 0.3455\n",
      "Epoch 3/30\n",
      "413/413 [==============================] - 0s 754us/step - loss: 0.3340 - mse: 0.3340 - val_loss: 0.3333 - val_mse: 0.3333\n",
      "Epoch 4/30\n",
      "413/413 [==============================] - 0s 742us/step - loss: 0.3240 - mse: 0.3240 - val_loss: 0.3548 - val_mse: 0.3548\n",
      "Epoch 5/30\n",
      "413/413 [==============================] - 0s 765us/step - loss: 0.3203 - mse: 0.3203 - val_loss: 0.3332 - val_mse: 0.3332\n",
      "Epoch 6/30\n",
      "413/413 [==============================] - 0s 757us/step - loss: 0.3094 - mse: 0.3094 - val_loss: 0.3249 - val_mse: 0.3249\n",
      "Epoch 7/30\n",
      "413/413 [==============================] - 0s 752us/step - loss: 0.3031 - mse: 0.3031 - val_loss: 0.3342 - val_mse: 0.3342\n",
      "Epoch 8/30\n",
      "413/413 [==============================] - 0s 804us/step - loss: 0.2971 - mse: 0.2971 - val_loss: 0.3934 - val_mse: 0.3934\n",
      "Epoch 9/30\n",
      "413/413 [==============================] - 0s 764us/step - loss: 0.2920 - mse: 0.2920 - val_loss: 0.3183 - val_mse: 0.3183\n",
      "Epoch 10/30\n",
      "413/413 [==============================] - 0s 774us/step - loss: 0.2851 - mse: 0.2851 - val_loss: 0.3305 - val_mse: 0.3305\n",
      "Epoch 11/30\n",
      "413/413 [==============================] - 0s 770us/step - loss: 0.2857 - mse: 0.2857 - val_loss: 0.3027 - val_mse: 0.3027\n",
      "Epoch 12/30\n",
      "413/413 [==============================] - 0s 774us/step - loss: 0.2811 - mse: 0.2811 - val_loss: 0.3004 - val_mse: 0.3004\n",
      "Epoch 13/30\n",
      "413/413 [==============================] - 0s 760us/step - loss: 0.2759 - mse: 0.2759 - val_loss: 0.3033 - val_mse: 0.3033\n",
      "Epoch 14/30\n",
      "413/413 [==============================] - 0s 776us/step - loss: 0.2750 - mse: 0.2750 - val_loss: 0.2986 - val_mse: 0.2986\n",
      "Epoch 15/30\n",
      "413/413 [==============================] - 0s 755us/step - loss: 0.2708 - mse: 0.2708 - val_loss: 0.3049 - val_mse: 0.3049\n",
      "Epoch 16/30\n",
      "413/413 [==============================] - 0s 765us/step - loss: 0.2689 - mse: 0.2689 - val_loss: 0.2882 - val_mse: 0.2882\n",
      "Epoch 17/30\n",
      "413/413 [==============================] - 0s 763us/step - loss: 0.2649 - mse: 0.2649 - val_loss: 0.2977 - val_mse: 0.2977\n",
      "Epoch 18/30\n",
      "413/413 [==============================] - 0s 749us/step - loss: 0.2642 - mse: 0.2642 - val_loss: 0.2864 - val_mse: 0.2864\n",
      "Epoch 19/30\n",
      "413/413 [==============================] - 0s 767us/step - loss: 0.2609 - mse: 0.2609 - val_loss: 0.2835 - val_mse: 0.2835\n",
      "Epoch 20/30\n",
      "413/413 [==============================] - 0s 781us/step - loss: 0.2580 - mse: 0.2580 - val_loss: 0.2900 - val_mse: 0.2900\n",
      "Epoch 21/30\n",
      "413/413 [==============================] - 0s 784us/step - loss: 0.2577 - mse: 0.2577 - val_loss: 0.2931 - val_mse: 0.2931\n",
      "Epoch 22/30\n",
      "413/413 [==============================] - 0s 768us/step - loss: 0.2568 - mse: 0.2568 - val_loss: 0.3198 - val_mse: 0.3198\n",
      "Epoch 23/30\n",
      "413/413 [==============================] - 0s 793us/step - loss: 0.2539 - mse: 0.2539 - val_loss: 0.3019 - val_mse: 0.3019\n",
      "Epoch 24/30\n",
      "413/413 [==============================] - 0s 779us/step - loss: 0.2523 - mse: 0.2523 - val_loss: 0.3049 - val_mse: 0.3049\n",
      "Epoch 25/30\n",
      "413/413 [==============================] - 0s 767us/step - loss: 0.2496 - mse: 0.2496 - val_loss: 0.3144 - val_mse: 0.3144\n",
      "Epoch 26/30\n",
      "413/413 [==============================] - 0s 755us/step - loss: 0.2480 - mse: 0.2480 - val_loss: 0.3067 - val_mse: 0.3067\n",
      "Epoch 27/30\n",
      "413/413 [==============================] - 0s 769us/step - loss: 0.2461 - mse: 0.2461 - val_loss: 0.3136 - val_mse: 0.3136\n",
      "Epoch 28/30\n",
      "413/413 [==============================] - 0s 749us/step - loss: 0.2441 - mse: 0.2441 - val_loss: 0.2981 - val_mse: 0.2981\n",
      "Epoch 29/30\n",
      "413/413 [==============================] - 0s 769us/step - loss: 0.2427 - mse: 0.2427 - val_loss: 0.2903 - val_mse: 0.2903\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomizedSearchCV(cv=3,\n                   estimator=<keras.wrappers.scikit_learn.KerasRegressor object at 0x000001677C747FD0>,\n                   param_distributions={'learning_rate': [0.001683454924600351,\n                                                          0.02390836445593178,\n                                                          0.008731907739399206,\n                                                          0.004725396149933917,\n                                                          0.0006154014789262348,\n                                                          0.0006153331256530192,\n                                                          0.0003920021771415983,\n                                                          0.01619845322936229,\n                                                          0.004779156784872302,\n                                                          0.007821074275112...\n                                                          0.005021425736625637,\n                                                          0.0005703073595961105,\n                                                          0.001151888789941251,\n                                                          0.001621231156394198,\n                                                          0.0024505367684280487,\n                                                          0.011155092541719619,\n                                                          0.0007524347058135697,\n                                                          0.0032032448128444043,\n                                                          0.004591455636549438,\n                                                          0.0003715541189658278, ...],\n                                        'n_hidden': [0, 1, 2, 3],\n                                        'n_neurons': [1, 2, 3, 4, 5, 6, 7, 8, 9,\n                                                      10, 11, 12, 13, 14, 15,\n                                                      16, 17, 18, 19, 20, 21,\n                                                      22, 23, 24, 25, 26, 27,\n                                                      28, 29, 30, ...]},\n                   verbose=2)"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "from scipy.stats import reciprocal\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_hidden\": [0, 1, 2, 3],\n",
    "    \"n_neurons\": np.arange(1, 100).tolist(),\n",
    "    \"learning_rate\": reciprocal(3e-4, 3e-2).rvs(1000).tolist(),\n",
    "}\n",
    "\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10, cv=3, verbose=2)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [1 mark]\n",
    "\n",
    "Plot the learning curves for the best model in (c). Does it look like the model is overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "{'n_neurons': 80, 'n_hidden': 3, 'learning_rate': 0.0059640580092043885}"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd_search_cv.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "413/413 [==============================] - 1s 979us/step - loss: 0.5139 - mse: 0.5139 - val_loss: 0.6386 - val_mse: 0.6386\n",
      "Epoch 2/30\n",
      "413/413 [==============================] - 0s 800us/step - loss: 0.3765 - mse: 0.3765 - val_loss: 0.3438 - val_mse: 0.3438\n",
      "Epoch 3/30\n",
      "413/413 [==============================] - 0s 772us/step - loss: 0.3413 - mse: 0.3413 - val_loss: 0.3357 - val_mse: 0.3357\n",
      "Epoch 4/30\n",
      "413/413 [==============================] - 0s 772us/step - loss: 0.3295 - mse: 0.3295 - val_loss: 0.3409 - val_mse: 0.3409\n",
      "Epoch 5/30\n",
      "413/413 [==============================] - 0s 767us/step - loss: 0.3317 - mse: 0.3317 - val_loss: 0.3203 - val_mse: 0.3203\n",
      "Epoch 6/30\n",
      "413/413 [==============================] - 0s 770us/step - loss: 0.3128 - mse: 0.3128 - val_loss: 0.3209 - val_mse: 0.3209\n",
      "Epoch 7/30\n",
      "413/413 [==============================] - 0s 760us/step - loss: 0.3106 - mse: 0.3106 - val_loss: 0.3355 - val_mse: 0.3355\n",
      "Epoch 8/30\n",
      "413/413 [==============================] - 0s 783us/step - loss: 0.3008 - mse: 0.3008 - val_loss: 0.3955 - val_mse: 0.3955\n",
      "Epoch 9/30\n",
      "413/413 [==============================] - 0s 775us/step - loss: 0.2924 - mse: 0.2924 - val_loss: 0.3278 - val_mse: 0.3278\n",
      "Epoch 10/30\n",
      "413/413 [==============================] - 0s 784us/step - loss: 0.2885 - mse: 0.2885 - val_loss: 0.3312 - val_mse: 0.3312\n",
      "Epoch 11/30\n",
      "413/413 [==============================] - 0s 779us/step - loss: 0.2877 - mse: 0.2877 - val_loss: 0.3062 - val_mse: 0.3062\n",
      "Epoch 12/30\n",
      "413/413 [==============================] - 0s 806us/step - loss: 0.2830 - mse: 0.2830 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 13/30\n",
      "413/413 [==============================] - 0s 773us/step - loss: 0.2778 - mse: 0.2778 - val_loss: 0.3110 - val_mse: 0.3110\n",
      "Epoch 14/30\n",
      "413/413 [==============================] - 0s 773us/step - loss: 0.2774 - mse: 0.2774 - val_loss: 0.2988 - val_mse: 0.2988\n",
      "Epoch 15/30\n",
      "413/413 [==============================] - 0s 760us/step - loss: 0.2733 - mse: 0.2733 - val_loss: 0.3010 - val_mse: 0.3010\n",
      "Epoch 16/30\n",
      "413/413 [==============================] - 0s 777us/step - loss: 0.2721 - mse: 0.2721 - val_loss: 0.2936 - val_mse: 0.2936\n",
      "Epoch 17/30\n",
      "413/413 [==============================] - 0s 790us/step - loss: 0.2685 - mse: 0.2685 - val_loss: 0.3134 - val_mse: 0.3134\n",
      "Epoch 18/30\n",
      "413/413 [==============================] - 0s 756us/step - loss: 0.2640 - mse: 0.2640 - val_loss: 0.2953 - val_mse: 0.2953\n",
      "Epoch 19/30\n",
      "413/413 [==============================] - 0s 776us/step - loss: 0.2638 - mse: 0.2638 - val_loss: 0.2854 - val_mse: 0.2854\n",
      "Epoch 20/30\n",
      "413/413 [==============================] - 0s 795us/step - loss: 0.2611 - mse: 0.2611 - val_loss: 0.2955 - val_mse: 0.2955\n",
      "Epoch 21/30\n",
      "413/413 [==============================] - 0s 779us/step - loss: 0.2574 - mse: 0.2574 - val_loss: 0.2976 - val_mse: 0.2976\n",
      "Epoch 22/30\n",
      "413/413 [==============================] - 0s 752us/step - loss: 0.2620 - mse: 0.2620 - val_loss: 0.2894 - val_mse: 0.2894\n",
      "Epoch 23/30\n",
      "413/413 [==============================] - 0s 772us/step - loss: 0.2558 - mse: 0.2558 - val_loss: 0.2935 - val_mse: 0.2935\n",
      "Epoch 24/30\n",
      "413/413 [==============================] - 0s 748us/step - loss: 0.2530 - mse: 0.2530 - val_loss: 0.3043 - val_mse: 0.3043\n",
      "Epoch 25/30\n",
      "413/413 [==============================] - 0s 807us/step - loss: 0.2492 - mse: 0.2492 - val_loss: 0.3230 - val_mse: 0.3230\n",
      "Epoch 26/30\n",
      "413/413 [==============================] - 0s 780us/step - loss: 0.2488 - mse: 0.2488 - val_loss: 0.3076 - val_mse: 0.3076\n",
      "Epoch 27/30\n",
      "413/413 [==============================] - 0s 758us/step - loss: 0.2474 - mse: 0.2474 - val_loss: 0.3093 - val_mse: 0.3093\n",
      "Epoch 28/30\n",
      "413/413 [==============================] - 0s 761us/step - loss: 0.2498 - mse: 0.2498 - val_loss: 0.2952 - val_mse: 0.2952\n",
      "Epoch 29/30\n",
      "413/413 [==============================] - 0s 758us/step - loss: 0.2443 - mse: 0.2443 - val_loss: 0.2985 - val_mse: 0.2985\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "reset_session()\n",
    "run = rnd_search_cv.best_estimator_.fit(X_train, y_train, epochs=30,\n",
    "                                        validation_data=(X_valid, y_valid),\n",
    "                                        callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
    "\n",
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Binary classification DNN [23 marks]\n",
    "\n",
    "Consider the [Caravan insurance data](https://liacs.leidenuniv.nl/~puttenpwhvander/library/cc2000/). The data gives sociodemographic and product ownership data from customers of an insurance company, some of which purchased caravan insurance. You can read ther data description from that website. Download the data as a csv file from [Canvas](https://canvas.uw.edu/files/92281351/download?download_frd=1).\n",
    "\n",
    "The target variable is `Purchase`, which is binary and you should convert it to 1 for `Yes` and 0 for `No`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [3 marks]\n",
    "\n",
    "Load, split and preprocess the data. In particular, for the splitting, you will need to create a test set (20% of the full data), a validation set (20% of the remaining data after creating the test set), and a training set, and stratify using the target variable because this is an imbalanced dataset. For both splits, use `random_state=42`. The features `MOSTYPE` and `MOSHOOFD` are categorical so one hot encoding needs to be applied to them. All other features are numerical so a standard scaler needs to be applied to them. Print the training set `X_train`, `y_train` using `print()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"caravan.csv\")\n",
    "data[\"Purchase\"] = (data[\"Purchase\"] == \"Yes\").astype(int)\n",
    "\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(data, data[\"Purchase\"]):\n",
    "    strat_train_set = data.loc[train_index]\n",
    "    strat_test_set = data.loc[test_index]\n",
    "\n",
    "split2 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index2, valid_index in split2.split(strat_train_set, strat_train_set[\"Purchase\"]):\n",
    "    strat_train2_set = strat_train_set.iloc[train_index2]\n",
    "    strat_valid_set = strat_train_set.iloc[valid_index]\n",
    "\n",
    "X_raw = strat_train2_set.drop(\"Purchase\", axis=1)\n",
    "y_train = strat_train2_set[\"Purchase\"]\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        ('std_scaler', StandardScaler()),\n",
    "    ])\n",
    "num_features = X_raw.drop([\"MOSTYPE\",\"MOSHOOFD\"], axis=1)\n",
    "num_attribs = list(num_features)\n",
    "cat_attribs = [\"MOSTYPE\",\"MOSHOOFD\"]\n",
    "full_pipeline = ColumnTransformer([\n",
    "        (\"num\", num_pipeline, num_attribs),\n",
    "        (\"cat\", OneHotEncoder(), cat_attribs),\n",
    "    ])\n",
    "\n",
    "X_train = full_pipeline.fit_transform(X_raw)\n",
    "\n",
    "X_valid_raw = strat_valid_set.drop(\"Purchase\", axis=1)\n",
    "y_valid = strat_valid_set[\"Purchase\"]\n",
    "X_valid = full_pipeline.transform(X_valid_raw)\n",
    "\n",
    "X_test_raw = strat_test_set.drop(\"Purchase\", axis=1)\n",
    "y_test = strat_test_set[\"Purchase\"]\n",
    "X_test = full_pipeline.transform(X_test_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.26920841  0.40025864  0.00630108 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.26920841 -0.87080994  1.24164497 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.26920841 -0.87080994  0.00630108 ...  1.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [-0.26920841  1.67132722  0.00630108 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.26920841  0.40025864 -1.2290428  ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.26920841  0.40025864  0.00630108 ...  1.          0.\n",
      "   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5356    0\n",
      "2968    0\n",
      "1180    0\n",
      "5182    0\n",
      "2623    0\n",
      "       ..\n",
      "219     0\n",
      "1701    0\n",
      "5360    0\n",
      "978     0\n",
      "4062    0\n",
      "Name: Purchase, Length: 3725, dtype: int32\n"
     ]
    }
   ],
   "source": [
    "print(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "\n",
    "In the next part (c), you will build and fit a DNN with 5 hidden layers of 200, 200, 100, 100, 50 neurons, respectively. Use the following specifications:\n",
    "\n",
    "(i) He initialization and the ELU activation function.\n",
    "\n",
    "(ii) The output layer has 1 neuron with sigmoid activation.\n",
    "\n",
    "(iii) Compile with `loss=\"binary_crossentropy\"` and  `metrics=[\"AUC\"]` (which is ROC AUC).\n",
    "\n",
    "In this question, explain why the choices (i), (ii), and (iii) are justified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "model_c = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_c.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_c.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_c.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [3 marks]\n",
    "\n",
    "Train the model in (b) for 30 epochs and use exponential scheduling and the NAG optimizer with `momentum=0.9`. Use a learning curve to comment on whether it is overfitting.\n",
    "\n",
    "At the start of fitting your model, run `reset_session()` given by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as python_random\n",
    "\n",
    "def reset_session(seed=42):\n",
    "    tf.random.set_seed(seed)\n",
    "    python_random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 0.2462 - auc: 0.6247 - val_loss: 0.2280 - val_auc: 0.6375 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1890 - auc: 0.7985 - val_loss: 0.2285 - val_auc: 0.6892 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1673 - auc: 0.8563 - val_loss: 0.2353 - val_auc: 0.6605 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1548 - auc: 0.8923 - val_loss: 0.2379 - val_auc: 0.6852 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1404 - auc: 0.9183 - val_loss: 0.2363 - val_auc: 0.6920 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1266 - auc: 0.9383 - val_loss: 0.2624 - val_auc: 0.6872 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1178 - auc: 0.9462 - val_loss: 0.2535 - val_auc: 0.6827 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1047 - auc: 0.9638 - val_loss: 0.2827 - val_auc: 0.6575 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1002 - auc: 0.9677 - val_loss: 0.2838 - val_auc: 0.6697 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0915 - auc: 0.9753 - val_loss: 0.2955 - val_auc: 0.6716 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0834 - auc: 0.9801 - val_loss: 0.2943 - val_auc: 0.6669 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0778 - auc: 0.9836 - val_loss: 0.3069 - val_auc: 0.6602 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0729 - auc: 0.9865 - val_loss: 0.3155 - val_auc: 0.6552 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0679 - auc: 0.9883 - val_loss: 0.3236 - val_auc: 0.6553 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0644 - auc: 0.9904 - val_loss: 0.3299 - val_auc: 0.6438 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0605 - auc: 0.9919 - val_loss: 0.3520 - val_auc: 0.6433 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0577 - auc: 0.9926 - val_loss: 0.3449 - val_auc: 0.6609 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0565 - auc: 0.9933 - val_loss: 0.3521 - val_auc: 0.6526 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0532 - auc: 0.9941 - val_loss: 0.3537 - val_auc: 0.6650 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0510 - auc: 0.9952 - val_loss: 0.3557 - val_auc: 0.6473 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0490 - auc: 0.9957 - val_loss: 0.3622 - val_auc: 0.6420 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0471 - auc: 0.9959 - val_loss: 0.3686 - val_auc: 0.6482 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0456 - auc: 0.9965 - val_loss: 0.3764 - val_auc: 0.6586 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0455 - auc: 0.9963 - val_loss: 0.3770 - val_auc: 0.6564 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0438 - auc: 0.9968 - val_loss: 0.3789 - val_auc: 0.6618 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0432 - auc: 0.9970 - val_loss: 0.3829 - val_auc: 0.6612 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0424 - auc: 0.9970 - val_loss: 0.3866 - val_auc: 0.6500 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0416 - auc: 0.9974 - val_loss: 0.3895 - val_auc: 0.6572 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0410 - auc: 0.9974 - val_loss: 0.3909 - val_auc: 0.6581 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0403 - auc: 0.9976 - val_loss: 0.3924 - val_auc: 0.6590 - lr: 3.5481e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_session()\n",
    "def exponential_decay(lr0, s):\n",
    "    return lambda epoch: lr0 * 0.1**(epoch / s)\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "\n",
    "run = model_c.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30/30 [==============================] - 0s 777us/step - loss: 0.3924 - auc: 0.6590\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.3923604488372803, 0.6589714288711548]"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_c.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [12 marks]\n",
    "\n",
    "Fit separate models using the same specification as in (c) but with the following regularization techniques:\n",
    "\n",
    "(i) batch normalization,\n",
    "\n",
    "(ii) early stopping based on validation AUC with `patience=10` (look at the documentation and note the `mode` argument).\n",
    "\n",
    "(iii) $\\ell_2$ regularization with `l2=0.001`,\n",
    "\n",
    "(iv) dropout with probability 0.2,\n",
    "\n",
    "(v) $\\ell_2$ regularization and early stopping both as above,\n",
    "\n",
    "(vi) batch normalization and dropout both as above.\n",
    "\n",
    "At the start of each one of the above models, run `reset_session()`.\n",
    "\n",
    "The performance measure is validation AUC. State this for the model in (c), and for each of the models here comment on whether it is better than the model in (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### batch normalization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "model_1 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_1.add(keras.layers.BatchNormalization())\n",
    "    model_1.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_1.add(keras.layers.BatchNormalization())\n",
    "model_1.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_1.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 2s 4ms/step - loss: 0.2996 - auc: 0.6198 - val_loss: 0.2203 - val_auc: 0.6598 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1969 - auc: 0.7746 - val_loss: 0.2269 - val_auc: 0.6643 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1781 - auc: 0.8368 - val_loss: 0.2295 - val_auc: 0.6644 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1708 - auc: 0.8523 - val_loss: 0.2371 - val_auc: 0.6488 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1557 - auc: 0.8902 - val_loss: 0.2486 - val_auc: 0.6516 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1461 - auc: 0.9118 - val_loss: 0.2481 - val_auc: 0.6674 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1414 - auc: 0.9170 - val_loss: 0.2525 - val_auc: 0.6607 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1270 - auc: 0.9388 - val_loss: 0.2635 - val_auc: 0.6369 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1296 - auc: 0.9353 - val_loss: 0.2691 - val_auc: 0.6667 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1145 - auc: 0.9553 - val_loss: 0.2757 - val_auc: 0.6795 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1147 - auc: 0.9549 - val_loss: 0.2784 - val_auc: 0.6435 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1125 - auc: 0.9559 - val_loss: 0.2821 - val_auc: 0.6592 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1045 - auc: 0.9657 - val_loss: 0.2840 - val_auc: 0.6391 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1015 - auc: 0.9675 - val_loss: 0.2962 - val_auc: 0.6519 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0971 - auc: 0.9714 - val_loss: 0.2967 - val_auc: 0.6312 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0978 - auc: 0.9705 - val_loss: 0.3018 - val_auc: 0.6355 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0899 - auc: 0.9769 - val_loss: 0.3093 - val_auc: 0.6401 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0900 - auc: 0.9751 - val_loss: 0.3082 - val_auc: 0.6358 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0826 - auc: 0.9825 - val_loss: 0.3080 - val_auc: 0.6413 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0838 - auc: 0.9808 - val_loss: 0.3113 - val_auc: 0.6446 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0843 - auc: 0.9805 - val_loss: 0.3157 - val_auc: 0.6309 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0787 - auc: 0.9826 - val_loss: 0.3214 - val_auc: 0.6425 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0850 - auc: 0.9786 - val_loss: 0.3219 - val_auc: 0.6315 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0748 - auc: 0.9854 - val_loss: 0.3231 - val_auc: 0.6303 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0768 - auc: 0.9815 - val_loss: 0.3266 - val_auc: 0.6323 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0797 - auc: 0.9816 - val_loss: 0.3262 - val_auc: 0.6395 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0769 - auc: 0.9850 - val_loss: 0.3283 - val_auc: 0.6272 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0745 - auc: 0.9866 - val_loss: 0.3262 - val_auc: 0.6347 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0772 - auc: 0.9838 - val_loss: 0.3265 - val_auc: 0.6387 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0746 - auc: 0.9857 - val_loss: 0.3294 - val_auc: 0.6282 - lr: 3.5481e-04\n",
      "30/30 [==============================] - 0s 970us/step - loss: 0.3294 - auc: 0.6282\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.32942235469818115, 0.6282310485839844]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "run = model_1.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "model_1.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### early stopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "model_2 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_2.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_2.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_2.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 0.2462 - auc: 0.6247 - val_loss: 0.2280 - val_auc: 0.6375 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1890 - auc: 0.7985 - val_loss: 0.2285 - val_auc: 0.6892 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1673 - auc: 0.8563 - val_loss: 0.2353 - val_auc: 0.6605 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1548 - auc: 0.8923 - val_loss: 0.2379 - val_auc: 0.6852 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1404 - auc: 0.9183 - val_loss: 0.2363 - val_auc: 0.6920 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1266 - auc: 0.9383 - val_loss: 0.2624 - val_auc: 0.6872 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1178 - auc: 0.9462 - val_loss: 0.2535 - val_auc: 0.6827 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1047 - auc: 0.9638 - val_loss: 0.2827 - val_auc: 0.6575 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1002 - auc: 0.9677 - val_loss: 0.2838 - val_auc: 0.6697 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0915 - auc: 0.9753 - val_loss: 0.2955 - val_auc: 0.6716 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0834 - auc: 0.9801 - val_loss: 0.2943 - val_auc: 0.6669 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0778 - auc: 0.9836 - val_loss: 0.3069 - val_auc: 0.6602 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0729 - auc: 0.9865 - val_loss: 0.3155 - val_auc: 0.6552 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0679 - auc: 0.9883 - val_loss: 0.3236 - val_auc: 0.6553 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0644 - auc: 0.9904 - val_loss: 0.3299 - val_auc: 0.6438 - lr: 0.0020\n",
      "30/30 [==============================] - 0s 750us/step - loss: 0.3299 - auc: 0.6438\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.3299334943294525, 0.6438356041908264]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\",\n",
    "                                                  patience=10,\n",
    "                                                  mode=\"max\")\n",
    "run = model_2.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler, early_stopping_cb])\n",
    "\n",
    "model_2.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $\\ell_2$ regularization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "model_3 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_3.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                                 kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model_3.add(keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_3.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "              metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 11.0016 - auc: 0.6298 - val_loss: 8.6342 - val_auc: 0.6415 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 7.0640 - auc: 0.7592 - val_loss: 5.7258 - val_auc: 0.6799 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 4.7906 - auc: 0.7793 - val_loss: 3.9987 - val_auc: 0.6809 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 3.4160 - auc: 0.7737 - val_loss: 2.9242 - val_auc: 0.6868 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 2.5456 - auc: 0.7769 - val_loss: 2.2308 - val_auc: 0.6845 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.9736 - auc: 0.7828 - val_loss: 1.7682 - val_auc: 0.6803 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.5860 - auc: 0.7831 - val_loss: 1.4474 - val_auc: 0.6817 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.3145 - auc: 0.7795 - val_loss: 1.2205 - val_auc: 0.6868 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.1198 - auc: 0.7789 - val_loss: 1.0545 - val_auc: 0.6804 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.9762 - auc: 0.7812 - val_loss: 0.9312 - val_auc: 0.6810 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.8685 - auc: 0.7796 - val_loss: 0.8377 - val_auc: 0.6797 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.7862 - auc: 0.7793 - val_loss: 0.7654 - val_auc: 0.6798 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.7218 - auc: 0.7792 - val_loss: 0.7088 - val_auc: 0.6812 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.6711 - auc: 0.7805 - val_loss: 0.6633 - val_auc: 0.6784 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.6305 - auc: 0.7792 - val_loss: 0.6270 - val_auc: 0.6807 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5975 - auc: 0.7801 - val_loss: 0.5973 - val_auc: 0.6789 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5705 - auc: 0.7824 - val_loss: 0.5730 - val_auc: 0.6784 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5483 - auc: 0.7820 - val_loss: 0.5526 - val_auc: 0.6810 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5297 - auc: 0.7808 - val_loss: 0.5357 - val_auc: 0.6797 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5142 - auc: 0.7794 - val_loss: 0.5213 - val_auc: 0.6799 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.5010 - auc: 0.7805 - val_loss: 0.5092 - val_auc: 0.6805 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4898 - auc: 0.7808 - val_loss: 0.4990 - val_auc: 0.6788 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4803 - auc: 0.7810 - val_loss: 0.4900 - val_auc: 0.6802 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4721 - auc: 0.7805 - val_loss: 0.4825 - val_auc: 0.6794 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4650 - auc: 0.7808 - val_loss: 0.4759 - val_auc: 0.6813 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4589 - auc: 0.7791 - val_loss: 0.4703 - val_auc: 0.6796 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4536 - auc: 0.7800 - val_loss: 0.4654 - val_auc: 0.6779 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4490 - auc: 0.7796 - val_loss: 0.4610 - val_auc: 0.6777 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4450 - auc: 0.7806 - val_loss: 0.4573 - val_auc: 0.6790 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.4415 - auc: 0.7804 - val_loss: 0.4540 - val_auc: 0.6783 - lr: 3.5481e-04\n",
      "30/30 [==============================] - 0s 789us/step - loss: 0.4540 - auc: 0.6783\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.4539940655231476, 0.6783064603805542]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "run = model_3.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "model_3.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### dropout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "model_4 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_4.add(keras.layers.Dropout(rate=0.2))\n",
    "    model_4.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_4.add(keras.layers.Dropout(rate=0.2))\n",
    "model_4.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_4.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 0.3058 - auc: 0.5553 - val_loss: 0.2244 - val_auc: 0.6413 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2307 - auc: 0.6401 - val_loss: 0.2250 - val_auc: 0.6630 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2138 - auc: 0.7120 - val_loss: 0.2220 - val_auc: 0.6764 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2178 - auc: 0.6871 - val_loss: 0.2192 - val_auc: 0.6644 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2179 - auc: 0.6808 - val_loss: 0.2178 - val_auc: 0.6809 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2103 - auc: 0.7148 - val_loss: 0.2211 - val_auc: 0.6811 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2117 - auc: 0.7114 - val_loss: 0.2174 - val_auc: 0.6842 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2078 - auc: 0.7324 - val_loss: 0.2220 - val_auc: 0.6795 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2054 - auc: 0.7398 - val_loss: 0.2209 - val_auc: 0.6816 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2037 - auc: 0.7490 - val_loss: 0.2208 - val_auc: 0.6846 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2080 - auc: 0.7281 - val_loss: 0.2194 - val_auc: 0.6873 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2029 - auc: 0.7536 - val_loss: 0.2223 - val_auc: 0.6837 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2018 - auc: 0.7550 - val_loss: 0.2191 - val_auc: 0.6924 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2032 - auc: 0.7482 - val_loss: 0.2170 - val_auc: 0.6935 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2028 - auc: 0.7490 - val_loss: 0.2217 - val_auc: 0.6862 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2027 - auc: 0.7498 - val_loss: 0.2204 - val_auc: 0.6859 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2006 - auc: 0.7603 - val_loss: 0.2192 - val_auc: 0.6894 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2015 - auc: 0.7546 - val_loss: 0.2190 - val_auc: 0.6902 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1952 - auc: 0.7825 - val_loss: 0.2199 - val_auc: 0.6852 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2002 - auc: 0.7663 - val_loss: 0.2182 - val_auc: 0.6923 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1987 - auc: 0.7690 - val_loss: 0.2191 - val_auc: 0.6929 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1993 - auc: 0.7624 - val_loss: 0.2190 - val_auc: 0.6924 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1961 - auc: 0.7803 - val_loss: 0.2200 - val_auc: 0.6902 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2025 - auc: 0.7496 - val_loss: 0.2186 - val_auc: 0.6938 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1982 - auc: 0.7668 - val_loss: 0.2191 - val_auc: 0.6932 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1928 - auc: 0.7884 - val_loss: 0.2201 - val_auc: 0.6923 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1962 - auc: 0.7733 - val_loss: 0.2198 - val_auc: 0.6956 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.2007 - auc: 0.7575 - val_loss: 0.2194 - val_auc: 0.6959 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1975 - auc: 0.7782 - val_loss: 0.2195 - val_auc: 0.6967 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1960 - auc: 0.7767 - val_loss: 0.2200 - val_auc: 0.6928 - lr: 3.5481e-04\n",
      "30/30 [==============================] - 0s 759us/step - loss: 0.2200 - auc: 0.6928\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.2199716717004776, 0.6928408741950989]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "run = model_4.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "model_4.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $\\ell_2$ regularization and early stopping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "model_5 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_5.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\",\n",
    "                                 kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "model_5.add(keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.l2(0.01)))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_5.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 1s 3ms/step - loss: 11.0016 - auc: 0.6298 - val_loss: 8.6342 - val_auc: 0.6415 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 7.0640 - auc: 0.7592 - val_loss: 5.7258 - val_auc: 0.6799 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 4.7906 - auc: 0.7793 - val_loss: 3.9987 - val_auc: 0.6809 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 3.4160 - auc: 0.7737 - val_loss: 2.9242 - val_auc: 0.6868 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 2.5456 - auc: 0.7769 - val_loss: 2.2308 - val_auc: 0.6845 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.9736 - auc: 0.7828 - val_loss: 1.7682 - val_auc: 0.6803 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.5860 - auc: 0.7831 - val_loss: 1.4474 - val_auc: 0.6817 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 1.3145 - auc: 0.7795 - val_loss: 1.2205 - val_auc: 0.6868 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 1.1198 - auc: 0.7789 - val_loss: 1.0545 - val_auc: 0.6804 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.9762 - auc: 0.7812 - val_loss: 0.9312 - val_auc: 0.6810 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.8685 - auc: 0.7796 - val_loss: 0.8377 - val_auc: 0.6797 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7862 - auc: 0.7793 - val_loss: 0.7654 - val_auc: 0.6798 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.7218 - auc: 0.7792 - val_loss: 0.7088 - val_auc: 0.6812 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.6711 - auc: 0.7805 - val_loss: 0.6633 - val_auc: 0.6784 - lr: 0.0022\n",
      "30/30 [==============================] - 0s 795us/step - loss: 0.6633 - auc: 0.6784\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.6632557511329651, 0.6784287095069885]"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_auc\",\n",
    "                                                  patience=10,\n",
    "                                                  mode=\"max\")\n",
    "run = model_5.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler, early_stopping_cb])\n",
    "\n",
    "model_5.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### batch normalization and dropout"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "model_6 = keras.models.Sequential()\n",
    "for n_hidden in (200, 200, 100, 100, 50):\n",
    "    model_6.add(keras.layers.BatchNormalization())\n",
    "    model_6.add(keras.layers.Dropout(rate=0.2))\n",
    "    model_6.add(keras.layers.Dense(n_hidden, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
    "model_6.add(keras.layers.BatchNormalization())\n",
    "model_6.add(keras.layers.Dropout(rate=0.2))\n",
    "model_6.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.001, momentum=0.9, nesterov=True)\n",
    "model_6.compile(loss=\"binary_crossentropy\", optimizer=optimizer,\n",
    "                metrics=[\"AUC\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 2s 4ms/step - loss: 0.3421 - auc: 0.5378 - val_loss: 0.2179 - val_auc: 0.6722 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2338 - auc: 0.5864 - val_loss: 0.2185 - val_auc: 0.6622 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2212 - auc: 0.6618 - val_loss: 0.2145 - val_auc: 0.6880 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2196 - auc: 0.6706 - val_loss: 0.2155 - val_auc: 0.6770 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2160 - auc: 0.6752 - val_loss: 0.2131 - val_auc: 0.6954 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2119 - auc: 0.6994 - val_loss: 0.2143 - val_auc: 0.6977 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2088 - auc: 0.7188 - val_loss: 0.2159 - val_auc: 0.6832 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2074 - auc: 0.7256 - val_loss: 0.2166 - val_auc: 0.6874 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2097 - auc: 0.7163 - val_loss: 0.2164 - val_auc: 0.6893 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2066 - auc: 0.7347 - val_loss: 0.2164 - val_auc: 0.6877 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2089 - auc: 0.7236 - val_loss: 0.2156 - val_auc: 0.6863 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2046 - auc: 0.7437 - val_loss: 0.2152 - val_auc: 0.6921 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2025 - auc: 0.7505 - val_loss: 0.2155 - val_auc: 0.6914 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2009 - auc: 0.7531 - val_loss: 0.2167 - val_auc: 0.6837 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1999 - auc: 0.7596 - val_loss: 0.2161 - val_auc: 0.6884 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2044 - auc: 0.7452 - val_loss: 0.2162 - val_auc: 0.6908 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2015 - auc: 0.7540 - val_loss: 0.2165 - val_auc: 0.6881 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2054 - auc: 0.7422 - val_loss: 0.2164 - val_auc: 0.6869 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2001 - auc: 0.7621 - val_loss: 0.2163 - val_auc: 0.6883 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2014 - auc: 0.7565 - val_loss: 0.2163 - val_auc: 0.6930 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2048 - auc: 0.7430 - val_loss: 0.2161 - val_auc: 0.6877 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1996 - auc: 0.7650 - val_loss: 0.2167 - val_auc: 0.6887 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1996 - auc: 0.7641 - val_loss: 0.2170 - val_auc: 0.6865 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1999 - auc: 0.7595 - val_loss: 0.2167 - val_auc: 0.6901 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2006 - auc: 0.7633 - val_loss: 0.2168 - val_auc: 0.6860 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2012 - auc: 0.7571 - val_loss: 0.2172 - val_auc: 0.6832 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1988 - auc: 0.7697 - val_loss: 0.2174 - val_auc: 0.6835 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1993 - auc: 0.7668 - val_loss: 0.2176 - val_auc: 0.6814 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1986 - auc: 0.7708 - val_loss: 0.2173 - val_auc: 0.6824 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.2016 - auc: 0.7544 - val_loss: 0.2174 - val_auc: 0.6830 - lr: 3.5481e-04\n",
      "30/30 [==============================] - 0s 922us/step - loss: 0.2174 - auc: 0.6830\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.21742799878120422, 0.6829541921615601]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay(lr0=0.01,s=20))\n",
    "run = model_6.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "model_6.evaluate(X_valid, y_valid)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (e) [1 mark]\n",
    "\n",
    "For the dropout model in (d)(iv) determine whether or not it is overfitting less than the model in (c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 0s 3ms/step - loss: 0.1695 - auc: 0.8574 - val_loss: 0.2283 - val_auc: 0.6803 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1777 - auc: 0.8290 - val_loss: 0.2469 - val_auc: 0.6680 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1770 - auc: 0.8358 - val_loss: 0.2375 - val_auc: 0.6991 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1855 - auc: 0.8121 - val_loss: 0.2224 - val_auc: 0.6767 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1925 - auc: 0.7910 - val_loss: 0.2202 - val_auc: 0.6924 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1863 - auc: 0.7996 - val_loss: 0.2278 - val_auc: 0.6984 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1902 - auc: 0.7942 - val_loss: 0.2191 - val_auc: 0.6976 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1880 - auc: 0.8065 - val_loss: 0.2265 - val_auc: 0.6937 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1881 - auc: 0.8089 - val_loss: 0.2238 - val_auc: 0.6939 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1854 - auc: 0.8155 - val_loss: 0.2230 - val_auc: 0.6946 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1937 - auc: 0.7840 - val_loss: 0.2242 - val_auc: 0.6927 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1867 - auc: 0.8105 - val_loss: 0.2280 - val_auc: 0.6858 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1857 - auc: 0.8124 - val_loss: 0.2255 - val_auc: 0.6948 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1877 - auc: 0.8090 - val_loss: 0.2221 - val_auc: 0.6949 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1884 - auc: 0.8023 - val_loss: 0.2261 - val_auc: 0.6892 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1882 - auc: 0.8042 - val_loss: 0.2256 - val_auc: 0.6927 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1905 - auc: 0.7965 - val_loss: 0.2246 - val_auc: 0.6880 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1915 - auc: 0.7928 - val_loss: 0.2238 - val_auc: 0.6888 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1832 - auc: 0.8208 - val_loss: 0.2254 - val_auc: 0.6917 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1894 - auc: 0.8052 - val_loss: 0.2232 - val_auc: 0.6896 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1872 - auc: 0.8064 - val_loss: 0.2242 - val_auc: 0.6895 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1901 - auc: 0.7997 - val_loss: 0.2236 - val_auc: 0.6924 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1847 - auc: 0.8223 - val_loss: 0.2242 - val_auc: 0.6946 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1938 - auc: 0.7821 - val_loss: 0.2233 - val_auc: 0.6925 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1881 - auc: 0.8069 - val_loss: 0.2237 - val_auc: 0.6902 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1815 - auc: 0.8247 - val_loss: 0.2250 - val_auc: 0.6922 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1859 - auc: 0.8145 - val_loss: 0.2247 - val_auc: 0.6946 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1916 - auc: 0.7882 - val_loss: 0.2242 - val_auc: 0.6940 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1860 - auc: 0.8198 - val_loss: 0.2245 - val_auc: 0.6950 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1835 - auc: 0.8213 - val_loss: 0.2252 - val_auc: 0.6927 - lr: 3.5481e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "run = model_4.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.1254 - auc: 0.9331 - val_loss: 0.3421 - val_auc: 0.6278 - lr: 0.0100\n",
      "Epoch 2/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.1031 - auc: 0.9635 - val_loss: 0.3589 - val_auc: 0.6873 - lr: 0.0089\n",
      "Epoch 3/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0831 - auc: 0.9777 - val_loss: 0.3820 - val_auc: 0.6296 - lr: 0.0079\n",
      "Epoch 4/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0716 - auc: 0.9865 - val_loss: 0.3692 - val_auc: 0.6471 - lr: 0.0071\n",
      "Epoch 5/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0560 - auc: 0.9922 - val_loss: 0.3919 - val_auc: 0.6518 - lr: 0.0063\n",
      "Epoch 6/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0516 - auc: 0.9927 - val_loss: 0.4399 - val_auc: 0.6224 - lr: 0.0056\n",
      "Epoch 7/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0475 - auc: 0.9931 - val_loss: 0.4250 - val_auc: 0.6004 - lr: 0.0050\n",
      "Epoch 8/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0429 - auc: 0.9953 - val_loss: 0.4630 - val_auc: 0.6015 - lr: 0.0045\n",
      "Epoch 9/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0434 - auc: 0.9953 - val_loss: 0.4553 - val_auc: 0.6282 - lr: 0.0040\n",
      "Epoch 10/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0389 - auc: 0.9962 - val_loss: 0.4690 - val_auc: 0.6073 - lr: 0.0035\n",
      "Epoch 11/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0357 - auc: 0.9970 - val_loss: 0.4571 - val_auc: 0.5952 - lr: 0.0032\n",
      "Epoch 12/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0344 - auc: 0.9973 - val_loss: 0.4769 - val_auc: 0.6328 - lr: 0.0028\n",
      "Epoch 13/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0332 - auc: 0.9974 - val_loss: 0.4728 - val_auc: 0.6159 - lr: 0.0025\n",
      "Epoch 14/30\n",
      "117/117 [==============================] - 0s 2ms/step - loss: 0.0309 - auc: 0.9977 - val_loss: 0.4836 - val_auc: 0.6259 - lr: 0.0022\n",
      "Epoch 15/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0289 - auc: 0.9981 - val_loss: 0.4873 - val_auc: 0.5945 - lr: 0.0020\n",
      "Epoch 16/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0285 - auc: 0.9981 - val_loss: 0.5071 - val_auc: 0.5928 - lr: 0.0018\n",
      "Epoch 17/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0274 - auc: 0.9982 - val_loss: 0.4970 - val_auc: 0.6122 - lr: 0.0016\n",
      "Epoch 18/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0269 - auc: 0.9983 - val_loss: 0.5048 - val_auc: 0.6013 - lr: 0.0014\n",
      "Epoch 19/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0261 - auc: 0.9983 - val_loss: 0.5045 - val_auc: 0.5932 - lr: 0.0013\n",
      "Epoch 20/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0249 - auc: 0.9986 - val_loss: 0.5056 - val_auc: 0.6021 - lr: 0.0011\n",
      "Epoch 21/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0245 - auc: 0.9986 - val_loss: 0.5088 - val_auc: 0.5953 - lr: 0.0010\n",
      "Epoch 22/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0238 - auc: 0.9987 - val_loss: 0.5125 - val_auc: 0.6023 - lr: 8.9125e-04\n",
      "Epoch 23/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0235 - auc: 0.9988 - val_loss: 0.5170 - val_auc: 0.5958 - lr: 7.9433e-04\n",
      "Epoch 24/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0232 - auc: 0.9987 - val_loss: 0.5170 - val_auc: 0.5979 - lr: 7.0795e-04\n",
      "Epoch 25/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0226 - auc: 0.9988 - val_loss: 0.5172 - val_auc: 0.6075 - lr: 6.3096e-04\n",
      "Epoch 26/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0227 - auc: 0.9988 - val_loss: 0.5214 - val_auc: 0.5969 - lr: 5.6234e-04\n",
      "Epoch 27/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0224 - auc: 0.9989 - val_loss: 0.5259 - val_auc: 0.5997 - lr: 5.0119e-04\n",
      "Epoch 28/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0220 - auc: 0.9989 - val_loss: 0.5251 - val_auc: 0.5918 - lr: 4.4668e-04\n",
      "Epoch 29/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0219 - auc: 0.9989 - val_loss: 0.5268 - val_auc: 0.5929 - lr: 3.9811e-04\n",
      "Epoch 30/30\n",
      "117/117 [==============================] - 0s 1ms/step - loss: 0.0215 - auc: 0.9990 - val_loss: 0.5280 - val_auc: 0.6001 - lr: 3.5481e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "run = model_c.fit(X_train, y_train, epochs=30,\n",
    "                  validation_data=(X_valid, y_valid),\n",
    "                  callbacks=[lr_scheduler])\n",
    "\n",
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (f) [1 mark]\n",
    "\n",
    "Of the models in (c) and (d), one would now choose the best model according to the performance metric (validation AUC) to evaluate on the test set. But instead, evaluate the model in (d)(v) on the test set in terms of the AUC and confusion matrix (regardless of whether it is the best model given your results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 0s 757us/step - loss: 0.6482 - auc: 0.7945\n"
     ]
    },
    {
     "data": {
      "text/plain": "[0.6482296586036682, 0.7945270538330078]"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "model_5.evaluate(X_test, y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(2, 2), dtype=int32, numpy=\narray([[1095,    0],\n       [  70,    0]])>"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.confusion_matrix(y_test, model_5.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Time series using machine learning [14 marks]\n",
    "\n",
    "Obtain daily values of the [Japan/U.S. Foreign Exchange Rate (DEXJPUS)](https://fred.stlouisfed.org/series/DEXJPUS) starting from Jan 1, 1990, to Jan 1, 2022, from FRED. This can be obtained using the code below or you can download the data as a csv file from [Canvas](https://canvas.uw.edu/files/92281350/download?download_frd=1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas_datareader as pdr\n",
    "from datetime import datetime\n",
    "data = pdr.get_data_fred('DEXJPUS', datetime(1990,1,1),datetime(2022,1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (a) [2 marks]\n",
    "\n",
    "Create a training set (before 2010), a validation set (Jan 2010 to Dec 2015), and a test set (the rest of the data). Turn the time series data into a supervised learning dataset where the features are the value of the exchange rate in the last 10 days inclusive of the current day, and the target is the value of the exchange rate in the next day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "outputs": [],
   "source": [
    "train = data[:'2009-12-31'].dropna()\n",
    "valid = data['2010-1-1':'2015-12-31'].dropna()\n",
    "test = data['2016-1-1':].dropna()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "outputs": [],
   "source": [
    "def ts_split(ts, feature_steps=8, target_steps=1):\n",
    "    ts = ts.values.flatten()\n",
    "    n_obs = len(ts) - feature_steps - target_steps + 1\n",
    "    X = np.array([ts[idx:idx + feature_steps] for idx in range(n_obs)])\n",
    "    y = np.array([ts[idx + feature_steps:idx + feature_steps + target_steps]\n",
    "                  for idx in range(n_obs)])\n",
    "    return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "outputs": [],
   "source": [
    "X_train, y_train = ts_split(train, feature_steps=10, target_steps=1)\n",
    "X_valid, y_valid = ts_split(valid, feature_steps=10, target_steps=1)\n",
    "X_test, y_test = ts_split(test, feature_steps=10, target_steps=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) [3 marks]\n",
    "\n",
    "Fit a random forest regressor to predict the value of the exchange rate in the next day. Using the test set, report the mean squared error and the accuracy for the movement direction.\n",
    "\n",
    "Hint: You can calculate the accuracy of the movement direction by determining what the actual movement direction is and comparing it to the movement direction corresponding to the predicted value of the exchange rate. For instance, the movement direction of the test set `X_test` and `y_test` where a strictly up movement is `True` can be computed as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "movement_test = X_test[:,-1] < y_test.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King48\\AppData\\Local\\Temp\\ipykernel_121048\\4272059818.py:5: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  rf_reg.fit(X_train, y_train)\n"
     ]
    },
    {
     "data": {
      "text/plain": "0.3738088196839274"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_jobs=-1, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "mean_squared_error(y_test, y_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "outputs": [],
   "source": [
    "movement_pred = X_test[:,-1] < y_pred.ravel()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5137861466039004"
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(movement_test, movement_pred)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) [4 marks]\n",
    "\n",
    "Repeat (b), but now fit a deep RNN with 2 recurrent layers of 20 and 20 neurons, and an output layer which is 1 dense neuron. Use 100 epochs and the Nadam optimizer. Comment on the result and the learning curve (the validation set is used for the learning curve)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 2s 3ms/step - loss: 12316.5762 - val_loss: 7888.1206\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 11368.5762 - val_loss: 7278.5903\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 10654.5889 - val_loss: 6730.1587\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 9994.9902 - val_loss: 6220.2866\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 9373.0195 - val_loss: 5741.7114\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8783.2744 - val_loss: 5291.3369\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8222.8760 - val_loss: 4867.1123\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7689.8506 - val_loss: 4467.5356\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7182.8135 - val_loss: 4091.6309\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 6700.3418 - val_loss: 3738.0208\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6241.3101 - val_loss: 3405.8125\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 5804.9268 - val_loss: 3094.2852\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5390.3218 - val_loss: 2802.4243\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4996.7490 - val_loss: 2529.8833\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4623.5566 - val_loss: 2276.1785\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4270.0605 - val_loss: 2040.1484\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3935.5203 - val_loss: 1821.4548\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3619.4106 - val_loss: 1619.5344\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3321.2939 - val_loss: 1433.8370\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3040.6057 - val_loss: 1263.8206\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2776.8477 - val_loss: 1108.9409\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2529.4795 - val_loss: 968.8696\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2297.9788 - val_loss: 842.8362\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2081.9360 - val_loss: 730.3357\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1880.8551 - val_loss: 631.0232\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1694.3062 - val_loss: 544.2772\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1521.7980 - val_loss: 469.5623\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1362.8517 - val_loss: 406.3795\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1217.0488 - val_loss: 354.0256\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1083.8638 - val_loss: 312.0430\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 962.8004 - val_loss: 279.8134\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 853.2881 - val_loss: 256.7076\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 754.8343 - val_loss: 241.9625\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 666.8702 - val_loss: 235.0058\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 588.8654 - val_loss: 235.0871\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 520.2083 - val_loss: 241.4903\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 460.3736 - val_loss: 253.4397\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 408.6965 - val_loss: 270.1349\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 364.4578 - val_loss: 290.7675\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 327.1049 - val_loss: 314.6956\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 295.9586 - val_loss: 340.8930\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 270.3738 - val_loss: 368.6588\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 249.6950 - val_loss: 397.2872\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 233.2795 - val_loss: 426.0062\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 220.4833 - val_loss: 454.2073\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 210.7315 - val_loss: 480.7908\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 203.4930 - val_loss: 506.0070\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 198.2442 - val_loss: 529.0931\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 194.5734 - val_loss: 550.0665\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 192.0528 - val_loss: 567.7250\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 190.3941 - val_loss: 583.1428\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 189.3621 - val_loss: 596.2869\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.7301 - val_loss: 606.5297\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.3810 - val_loss: 614.2939\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.1818 - val_loss: 620.7461\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.0666 - val_loss: 624.9208\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.0023 - val_loss: 628.8124\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.9797 - val_loss: 631.7641\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.9675 - val_loss: 634.0831\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.9692 - val_loss: 633.9179\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.9644 - val_loss: 634.1074\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 174.4148 - val_loss: 318.7491\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 109.7821 - val_loss: 70.7440\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 73.9398 - val_loss: 37.4416\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 58.8328 - val_loss: 41.2496\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 48.4306 - val_loss: 11.6781\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 40.0765 - val_loss: 11.6396\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 33.7449 - val_loss: 11.0097\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 28.6927 - val_loss: 3.8174\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 24.6299 - val_loss: 3.9492\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 21.4817 - val_loss: 2.2262\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 18.7522 - val_loss: 3.2723\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 16.5728 - val_loss: 3.3514\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 14.5373 - val_loss: 3.4064\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 13.1397 - val_loss: 5.4219\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 11.6281 - val_loss: 6.3567\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 10.5171 - val_loss: 4.4219\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 9.4422 - val_loss: 11.4854\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8.4878 - val_loss: 3.3121\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7.8056 - val_loss: 5.2414\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.9175 - val_loss: 4.4729\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.3138 - val_loss: 2.6386\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.7674 - val_loss: 3.1706\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.0901 - val_loss: 3.0177\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.7118 - val_loss: 4.9580\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.3500 - val_loss: 3.9475\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.0050 - val_loss: 2.5692\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.5474 - val_loss: 2.1973\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.2552 - val_loss: 1.9960\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.0186 - val_loss: 3.6360\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.7758 - val_loss: 2.0021\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.5890 - val_loss: 3.5822\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.4517 - val_loss: 2.4399\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.2100 - val_loss: 1.9110\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2.0298 - val_loss: 3.0556\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 1.9408 - val_loss: 5.2699\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.7520 - val_loss: 2.3970\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.7792 - val_loss: 3.4253\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.6807 - val_loss: 2.3786\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1.5943 - val_loss: 2.5750\n"
     ]
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "run = model.fit(X_train[..., np.newaxis], y_train, epochs=100,\n",
    "                validation_data=(X_valid[..., np.newaxis], y_valid))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (d) [5 marks]\n",
    "\n",
    "Create a supervised learning dataset suitable for predicting 3 days ahead instead of 1 day ahead. Adjust the deep RNN in (c) so that it predicts 3 days ahead. Use 100 epochs and the Nadam optimizer. Using the test set, report the mean squared error and the accuracy for the movement direction for each of the 3 days ahead predictions.  Comment on the result and the learning curve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Add your solution here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "outputs": [],
   "source": [
    "X_train_3ahead, y_train_3ahead = ts_split(train, feature_steps=10, target_steps=3)\n",
    "X_valid_3ahead, y_valid_3ahead = ts_split(valid, feature_steps=10, target_steps=3)\n",
    "X_test_3ahead, y_test_3ahead = ts_split(test, feature_steps=10, target_steps=3)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "157/157 [==============================] - 2s 3ms/step - loss: 12311.9678 - val_loss: 7877.4678\n",
      "Epoch 2/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 11363.8887 - val_loss: 7277.3262\n",
      "Epoch 3/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 10657.6963 - val_loss: 6731.9331\n",
      "Epoch 4/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 10000.3809 - val_loss: 6223.1577\n",
      "Epoch 5/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 9379.6641 - val_loss: 5745.3638\n",
      "Epoch 6/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8790.7266 - val_loss: 5295.4888\n",
      "Epoch 7/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8230.8408 - val_loss: 4871.6479\n",
      "Epoch 8/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7698.0371 - val_loss: 4472.1851\n",
      "Epoch 9/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7190.8887 - val_loss: 4096.1592\n",
      "Epoch 10/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6708.3042 - val_loss: 3742.5288\n",
      "Epoch 11/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6249.2236 - val_loss: 3410.1951\n",
      "Epoch 12/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5812.7290 - val_loss: 3098.4873\n",
      "Epoch 13/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5398.0103 - val_loss: 2806.7048\n",
      "Epoch 14/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5004.2637 - val_loss: 2534.0061\n",
      "Epoch 15/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4630.7598 - val_loss: 2279.8928\n",
      "Epoch 16/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4276.8599 - val_loss: 2043.6361\n",
      "Epoch 17/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3942.0359 - val_loss: 1824.6783\n",
      "Epoch 18/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3625.6738 - val_loss: 1622.5962\n",
      "Epoch 19/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3327.1921 - val_loss: 1436.5696\n",
      "Epoch 20/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3046.1440 - val_loss: 1266.3513\n",
      "Epoch 21/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2782.0273 - val_loss: 1111.2782\n",
      "Epoch 22/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2534.2681 - val_loss: 970.8506\n",
      "Epoch 23/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2302.4817 - val_loss: 844.5403\n",
      "Epoch 24/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 2086.1782 - val_loss: 731.9152\n",
      "Epoch 25/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1884.9036 - val_loss: 632.3788\n",
      "Epoch 26/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1698.1729 - val_loss: 545.5359\n",
      "Epoch 27/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1525.4419 - val_loss: 470.6048\n",
      "Epoch 28/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1366.2268 - val_loss: 407.2615\n",
      "Epoch 29/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1220.1196 - val_loss: 354.7364\n",
      "Epoch 30/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 1086.6224 - val_loss: 312.6095\n",
      "Epoch 31/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 965.2412 - val_loss: 280.1461\n",
      "Epoch 32/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 855.4983 - val_loss: 256.8735\n",
      "Epoch 33/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 756.7521 - val_loss: 242.0383\n",
      "Epoch 34/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 668.5304 - val_loss: 234.9400\n",
      "Epoch 35/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 590.3054 - val_loss: 234.9178\n",
      "Epoch 36/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 521.4955 - val_loss: 241.2117\n",
      "Epoch 37/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 461.4316 - val_loss: 253.0795\n",
      "Epoch 38/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 409.5401 - val_loss: 269.7439\n",
      "Epoch 39/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 365.2108 - val_loss: 290.3337\n",
      "Epoch 40/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 327.7553 - val_loss: 314.1703\n",
      "Epoch 41/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 296.4829 - val_loss: 340.3545\n",
      "Epoch 42/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 270.7852 - val_loss: 368.2001\n",
      "Epoch 43/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 250.0034 - val_loss: 396.7899\n",
      "Epoch 44/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 233.4859 - val_loss: 425.3801\n",
      "Epoch 45/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 220.6472 - val_loss: 453.3678\n",
      "Epoch 46/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 210.8197 - val_loss: 480.2363\n",
      "Epoch 47/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 203.4723 - val_loss: 505.5889\n",
      "Epoch 48/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 198.1528 - val_loss: 528.4164\n",
      "Epoch 49/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 194.4228 - val_loss: 549.7059\n",
      "Epoch 50/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 191.8600 - val_loss: 567.9273\n",
      "Epoch 51/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 190.1956 - val_loss: 583.6423\n",
      "Epoch 52/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 189.1543 - val_loss: 596.6828\n",
      "Epoch 53/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.5231 - val_loss: 606.6639\n",
      "Epoch 54/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 188.1576 - val_loss: 614.7512\n",
      "Epoch 55/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.9533 - val_loss: 621.5222\n",
      "Epoch 56/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.8498 - val_loss: 625.9824\n",
      "Epoch 57/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7945 - val_loss: 629.6497\n",
      "Epoch 58/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7769 - val_loss: 631.8322\n",
      "Epoch 59/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7610 - val_loss: 634.4072\n",
      "Epoch 60/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7610 - val_loss: 635.4656\n",
      "Epoch 61/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7573 - val_loss: 635.1078\n",
      "Epoch 62/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7605 - val_loss: 634.7884\n",
      "Epoch 63/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7780 - val_loss: 635.8700\n",
      "Epoch 64/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7596 - val_loss: 635.6506\n",
      "Epoch 65/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7697 - val_loss: 635.4380\n",
      "Epoch 66/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7626 - val_loss: 634.8766\n",
      "Epoch 67/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 187.7691 - val_loss: 634.2450\n",
      "Epoch 68/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 187.7659 - val_loss: 633.5652\n",
      "Epoch 69/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 177.9274 - val_loss: 236.7198\n",
      "Epoch 70/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 97.1445 - val_loss: 101.8170\n",
      "Epoch 71/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 66.7231 - val_loss: 50.5048\n",
      "Epoch 72/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 50.3684 - val_loss: 54.1196\n",
      "Epoch 73/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 40.8290 - val_loss: 24.7022\n",
      "Epoch 74/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 34.1379 - val_loss: 29.8153\n",
      "Epoch 75/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 28.1746 - val_loss: 26.8643\n",
      "Epoch 76/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 24.3121 - val_loss: 13.2001\n",
      "Epoch 77/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 20.9242 - val_loss: 10.5799\n",
      "Epoch 78/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 18.3694 - val_loss: 12.0631\n",
      "Epoch 79/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 16.2342 - val_loss: 8.3507\n",
      "Epoch 80/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 14.4663 - val_loss: 3.2816\n",
      "Epoch 81/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 13.0183 - val_loss: 8.7425\n",
      "Epoch 82/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 11.7830 - val_loss: 8.2473\n",
      "Epoch 83/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 10.4971 - val_loss: 3.8850\n",
      "Epoch 84/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 9.6409 - val_loss: 2.4629\n",
      "Epoch 85/100\n",
      "157/157 [==============================] - 0s 3ms/step - loss: 9.0736 - val_loss: 3.8137\n",
      "Epoch 86/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 8.1509 - val_loss: 2.3483\n",
      "Epoch 87/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 7.4301 - val_loss: 2.0813\n",
      "Epoch 88/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.7629 - val_loss: 3.0633\n",
      "Epoch 89/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.4109 - val_loss: 1.8212\n",
      "Epoch 90/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 6.0154 - val_loss: 1.6462\n",
      "Epoch 91/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.5466 - val_loss: 4.8577\n",
      "Epoch 92/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.1821 - val_loss: 1.5579\n",
      "Epoch 93/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 5.0426 - val_loss: 2.1969\n",
      "Epoch 94/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.5510 - val_loss: 1.7320\n",
      "Epoch 95/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.3494 - val_loss: 5.2219\n",
      "Epoch 96/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 4.1596 - val_loss: 2.2064\n",
      "Epoch 97/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.9683 - val_loss: 2.0856\n",
      "Epoch 98/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.7923 - val_loss: 2.3127\n",
      "Epoch 99/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.6755 - val_loss: 1.4853\n",
      "Epoch 100/100\n",
      "157/157 [==============================] - 0s 2ms/step - loss: 3.4988 - val_loss: 2.5799\n"
     ]
    }
   ],
   "source": [
    "reset_session()\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True, input_shape=[None, 1]),\n",
    "    keras.layers.SimpleRNN(20, return_sequences=True),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(1)),\n",
    "    keras.layers.Lambda(lambda Y_pred: Y_pred[:, -3:])\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"nadam\")\n",
    "\n",
    "run = model.fit(X_train_3ahead[..., np.newaxis],\n",
    "                y_train_3ahead[..., np.newaxis],\n",
    "                epochs=100,\n",
    "                validation_data=(X_valid_3ahead[..., np.newaxis],\n",
    "                                 y_valid_3ahead[..., np.newaxis]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 576x360 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(run.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.11196707, 1.12583179, 1.13190437])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypred = model.predict(X_test_3ahead[...,np.newaxis])[...,0]\n",
    "mean_squared_error(y_test_3ahead, ypred, multioutput=\"raw_values\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "data": {
      "text/plain": "0.49158249158249157"
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movement_test_1ahead = X_test_3ahead[:,-1] < y_test_3ahead[:,0].ravel()\n",
    "movement_pred_1ahead = X_test_3ahead[:,-1] < ypred[:,0].ravel()\n",
    "accuracy_score(movement_test_1ahead, movement_pred_1ahead)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "0.4909090909090909"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movement_test_2ahead = X_test_3ahead[:,-1] < y_test_3ahead[:,1].ravel()\n",
    "movement_pred_2ahead = X_test_3ahead[:,-1] < ypred[:,1].ravel()\n",
    "accuracy_score(movement_test_2ahead, movement_pred_2ahead)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [
    {
     "data": {
      "text/plain": "0.5151515151515151"
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movement_test_3ahead = X_test_3ahead[:,-1] < y_test_3ahead[:,2].ravel()\n",
    "movement_pred_3ahead = X_test_3ahead[:,-1] < ypred[:,2].ravel()\n",
    "accuracy_score(movement_test_3ahead, movement_pred_3ahead)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}